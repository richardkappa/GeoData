{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\GeoData\\lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Polygon\n",
    "import KeyFunctions as kf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/GeoData/\"\n",
    "Main_CRS = \"EPSG:27700\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into a standard structrue for the geopandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data\n",
    "rail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\n",
    "rail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "ferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\n",
    "ferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "bus_infile = root_path + \"NaPTANcsv/Stops.csv\"\n",
    "bus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "acc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\n",
    "acc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\n",
    "\n",
    "NSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\n",
    "NSPL_inputs = {\"Type\":\"Postcodes\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\n",
    "\n",
    "#NSUL_Infile defined below\n",
    "NSUL_inputs = {\"Type\":\"UDPRNs\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\n",
    "\n",
    "point_pairs = [[rail_infile, rail_inputs],\n",
    "[ferry_infile, ferry_inputs],\n",
    "[bus_infile, bus_inputs],\n",
    "[acc2020_infile, acc2020_inputs],\n",
    "[NSPL_infile,NSPL_inputs]]\n",
    "\n",
    "NSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\n",
    "\n",
    "point_pairs = point_pairs + NSUL_Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate the standard structure\n",
    "Points_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\n",
    "dtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\n",
    "Points_of_Interest = Points_of_Interest.astype(dtypes)\n",
    "Points_of_Interest = Points_of_Interest.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in point_pairs:\n",
    "    Points_of_Interest = Points_of_Interest.append(kf.Import_Points(l[0], l[1], dtypes), ignore_index=True)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"].astype(str).str.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\n",
    "LReg_Names = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', \n",
    "              'Old_New', 'Duration', 'PAON', 'SAON', 'Street', 'Locality', 'Town_City', 'District', 'County', \n",
    "              'PPDCategory_Type', 'Record_Status_monthly_file_only']\n",
    "\n",
    "usecols = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', 'Old_New', 'Duration']\n",
    "\n",
    "LReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \n",
    "LReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\n",
    "\n",
    "LReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "LReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "LReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "LReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\n",
    "    LReg_Data[\"Property_Type\"].to_numpy(), \n",
    "    LReg_Data[\"Old_New\"].to_numpy(), \n",
    "    LReg_Data[\"Duration\"].to_numpy())\n",
    "    ).tolist()\n",
    "\n",
    "LReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\n",
    "LReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\n",
    "LReg_Data[\"Type\"] = \"LReg\"\n",
    "LReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\n",
    "del LReg_Data, LReg_Data2020, LReg_Data2021, LReg_Names, usecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firestation Data\n",
    "FireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \n",
    "                         engine=\"odf\",\n",
    "                        sheet_name = \"STATIONS\")\n",
    "\n",
    "FireStations[\"Type\"] = \"FireStations\"\n",
    "FireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\n",
    "FireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\n",
    "FireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\n",
    "FireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\n",
    "del FireStations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schools Data\n",
    "usecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\n",
    "Schools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Schools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "Schools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "Schools[\"Type\"] = \"Schools\"\n",
    "Schools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\n",
    "Schools[\"Name\"] = Schools[\"EstablishmentName\"]\n",
    "Schools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\n",
    "del Schools, usecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computer doesn't have enough ram to deal with UDPRN data so need to drop it here\n",
    "Points_of_Interest = Points_of_Interest[Points_of_Interest[\"Type\"]!=\"UDPRNs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We know that some of the easings and northings are on the Irish grid, split these out\n",
    "NI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"Postcodes\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"UDPRNs\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\n",
    "\n",
    "NI = Points_of_Interest.loc[NI_Mask,:]\n",
    "Points_of_Interest = Points_of_Interest.loc[~NI_Mask,:]\n",
    "\n",
    "del NI_Mask_1, NI_Mask_2, NI_Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Geopandas dataframe\n",
    "x_points = Points_of_Interest[\"Easting\"].to_numpy()\n",
    "y_points = Points_of_Interest[\"Northing\"].to_numpy()\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\n",
    "del Points_of_Interest, x_points, y_points\n",
    "\n",
    "#Do the same for NI, and convert to the appropriate crs\n",
    "x_points = NI[\"Easting\"].to_numpy()\n",
    "y_points = NI[\"Northing\"].to_numpy()\n",
    "\n",
    "NI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\n",
    "NI_gdf = NI_gdf.to_crs(Main_CRS)\n",
    "del NI, x_points, y_points\n",
    "\n",
    "#Re-combine now they are on the same coordinates\n",
    "raw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, NI_gdf], ignore_index=True) )\n",
    "del NI_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Police Crime data\n",
    "usecols = [\"Crime type\", \"Month\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64, \"Month\":\"str\"}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"street.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "Crimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\n",
    "\n",
    "for file in file_list[1:]:\n",
    "    Crimes=Crimes.append(pd.read_csv(file, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\n",
    "\n",
    "Crimes = Crimes.dropna()\n",
    "Crimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\n",
    "Crimes = Crimes.rename(columns={\"Month\": \"Details_Str\"})\n",
    "Crimes[\"Type\"] = \"Crimes\"\n",
    "\n",
    "#Police Stop and Search Data\n",
    "usecols = [\"Object of search\", \"Date\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"search.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "StopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"])\n",
    "\n",
    "for file in file_list[1:]:\n",
    "    StopAndSearch=StopAndSearch.append(pd.read_csv(file, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"]), ignore_index=True)\n",
    "\n",
    "StopAndSearch = StopAndSearch.dropna()\n",
    "StopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\n",
    "StopAndSearch[\"Date\"] = StopAndSearch[\"Date\"].dt.strftime('%Y-%m-%d')\n",
    "StopAndSearch = StopAndSearch.rename(columns={\"Date\": \"Details_Str\"})\n",
    "StopAndSearch[\"Type\"] = \"StopAndSearch\"\n",
    "\n",
    "Crimes = Crimes.append(StopAndSearch,ignore_index=True)\n",
    "del StopAndSearch\n",
    "\n",
    "#Convert to Geopandas dataframe\n",
    "x_points = Crimes[\"Longitude\"].to_numpy()\n",
    "y_points = Crimes[\"Latitude\"].to_numpy()\n",
    "\n",
    "crimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\", \"Details_Str\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\n",
    "crimes_gdf = crimes_gdf.to_crs(Main_CRS)\n",
    "crimes_gdf\n",
    "\n",
    "del Crimes\n",
    "\n",
    "#combine with the main points gdf\n",
    "raw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, crimes_gdf], ignore_index=True) )\n",
    "del crimes_gdf, file_list, infile, usecols, in_dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NHS Data\n",
    "\n",
    "usecols = [\"OrganisationType\", \"OrganisationName\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"OrganisationType\":\"str\", \"OrganisationName\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "NHS_list = [root_path + \"NHS/\" + str(i) for i in os.listdir(\"D:/GeoData/NHS\") if \".csv\" in i]\n",
    "\n",
    "NHS = pd.read_csv(NHS_list[0], sep='¬'  , engine='python', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes)\n",
    "\n",
    "for file in NHS_list[1:]:\n",
    "    NHS = NHS.append(pd.read_csv(file, sep='¬'  , engine='python', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes), ignore_index=True)\n",
    "\n",
    "NHS = NHS.rename(columns={\"OrganisationType\":\"Type\", \"OrganisationName\":\"Name\"})\n",
    "\n",
    "#Convert to Geopandas dataframe\n",
    "x_points = NHS[\"Longitude\"].to_numpy()\n",
    "y_points = NHS[\"Latitude\"].to_numpy()\n",
    "\n",
    "NHS_gdf = gpd.GeoDataFrame(NHS.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\n",
    "NHS_gdf = NHS_gdf.to_crs(Main_CRS)\n",
    "NHS_gdf\n",
    "\n",
    "del NHS\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, NHS_gdf], ignore_index=True) )\n",
    "\n",
    "del NHS_gdf, usecols, in_dtypes, NHS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shape_Loc = {\n",
    "    'All_GB' : [root_path + \"Countries__December_2019__Boundaries_UK_BFC-shp/Countries__December_2019__Boundaries_UK_BFC.shp\", \"ctry19nm\"],\n",
    "    'National_Parks' : [root_path + \"National_Parks__December_2019__GB_BFE-shp/National_Parks__December_2019__GB_BFE.shp\", \"NPARK19NM\"],\n",
    "    'LocalAuthorities' : [root_path + \"Local_Authority_Districts__May_2020__Boundaries_UK_BFC-shp/Local_Authority_Districts__May_2020__Boundaries_UK_BFC.shp\", \"LAD20NM\"],\n",
    "    'LSOA' : [root_path + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2-shp/Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.shp\", \"LSOA11NMW\"],\n",
    "    'GreenSpace' : [root_path + \"opgrsp_essh_gb/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp\", \"id\"],\n",
    "    'Rivers' : [root_path + \"oprvrs_essh_gb/data/WatercourseLink.shp\",\"name1\"],\n",
    "    'Railway_Lines' : [root_path + \"strtgi_essh_gb/data/railway_line.shp\",\"LEGEND\"],\n",
    "    'Woodland_Region' : [root_path + \"strtgi_essh_gb/data/woodland_region.shp\",\"LEGEND\"],\n",
    "    'Urban_Region' : [root_path + \"strtgi_essh_gb/data/urban_region.shp\",\"LEGEND\"],\n",
    "    'Foreshor_Region' : [root_path + \"strtgi_essh_gb/data/foreshor_region.shp\",\"LEGEND\"],\n",
    "    'Ferry_Line' : [root_path + \"strtgi_essh_gb/data/ferry_line.shp\",\"LEGEND\", [\"FERRY_TIME\", \"FERRY_TYPE\", \"RESTRICTIO\", \"ACCESS\"]],\n",
    "    'Coastline' : [root_path + \"strtgi_essh_gb/data/coastline.shp\",\"LEGEND\"],\n",
    "    'Lakes' : [root_path + \"strtgi_essh_gb/data/lakes_region.shp\", \"LEGEND\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all of the shape files to the master list\n",
    "\n",
    "for k in Shape_Loc.keys():\n",
    "    v = Shape_Loc[k]\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, kf.get_shapefile(k, v, Main_CRS)], ignore_index=True) )\n",
    "\n",
    "del Shape_Loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the roads\n",
    "file = os.listdir(\"D:\\GeoData\\oproad_essh_gb\\data\")\n",
    "ignore_cols = [\"fictitious\", \"identifier\", \"name1_lang\", \"name2\", \"name2_lang\", \"formOfWay\", \"length\", \"primary\", \"trunkRoad\", \"loop\", \"startNode\", \"endNode\", \"structure\", \"nameTOID\", \"numberTOID\", \"function\"]\n",
    "\n",
    "path = [os.path.join(\"D:\\GeoData\\oproad_essh_gb\\data\", i) for i in file if \"RoadLink.shp\" in i]\n",
    "\n",
    "Roads = gpd.GeoDataFrame(pd.concat([gpd.read_file(i, ignore_fields=ignore_cols) for i in path], ignore_index=True), crs=Main_CRS)\n",
    "\n",
    "Roads[\"Type\"] = \"Road\"\n",
    "Roads = Roads.rename(columns={\"class\": \"Name\"})\n",
    "Roads = Roads.rename(columns={\"roadNumber\": \"Details_Str\"})\n",
    "Roads.loc[Roads[\"Details_Str\"].isnull(),\"Details_Str\"] = Roads[\"name1\"]\n",
    "Roads = Roads.drop(columns=[\"name1\"])\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, Roads], ignore_index=True) )\n",
    "del Roads, ignore_cols, path\n",
    "\n",
    "#Add the motorway junctions\n",
    "path = [os.path.join(\"D:\\GeoData\\oproad_essh_gb\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\n",
    "\n",
    "MotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\n",
    "\n",
    "MotorwayJunctions = MotorwayJunctions.drop(columns=\"identifier\")\n",
    "MotorwayJunctions = MotorwayJunctions.rename(columns={\"number\": \"Name\"})\n",
    "MotorwayJunctions[\"Type\"] = \"MotorwayJunction\"\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\n",
    "del MotorwayJunctions, path, file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural England data\n",
    "NE_Data = {\n",
    "    \"NE_Peat\" : {\"url\":\"https://opendata.arcgis.com/datasets/55b21c31a61c4292a465d7618d831eb8_0.geojson\", \"Type\":\"Peat\", \"Name\":\"PCLASSDESC\", \"Details_Str\":\"Type\", \"geometry\":\"geometry\"},\n",
    "    \"NE_SSI\" : {\"url\":\"https://opendata.arcgis.com/datasets/03fd7a2f8e4e4346bf41ce7879153949_0.geojson\", \"Type\":\"SiteOfScientificInterest\", \"Name\":\"SSSI_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_LocalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/b1d690ac6dd54c15bdd2d341b686ecd7_0.geojson\", \"Type\":\"LocalNatureReserve\", \"Name\":\"LNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_NationalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/ab7bfd86f5b347df8d47fc9bfab80caf_0.geojson\", \"Type\":\"NationalNatureReserve\", \"Name\":\"NNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_SpecialAreaOfConvervation\" : {\"url\":\"https://opendata.arcgis.com/datasets/e4142658906c498fa37f0a20d3fdfcff_0.geojson\", \"Type\":\"SpecialAreaOfConservation\", \"Name\":\"SAC_NAME\", \"Details_Str\":\"SAC_CODE\", \"geometry\":\"geometry\"},\n",
    "    \"NE_AncientWoodland\" : {\"url\":\"https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson\", \"Type\":\"AncientWoodland\", \"Name\":\"NAME\", \"Details_Str\":\"THEMNAME\", \"geometry\":\"geometry\"}\n",
    "}\n",
    "\n",
    "def get_NE_Data(details):\n",
    "    url = details[\"url\"]\n",
    "\n",
    "    usecols = [details[\"Name\"], details[\"Details_Str\"], details[\"geometry\"]]\n",
    "\n",
    "    gdf = gpd.read_file(url).to_crs(Main_CRS).loc[:, usecols]\n",
    "\n",
    "\n",
    "    gdf = gdf.rename(columns={details[\"Name\"]: \"Name\"})\n",
    "    gdf = gdf.rename(columns={details[\"Details_Str\"]: \"Details_Str\"})\n",
    "    gdf = gdf.rename(columns={details[\"geometry\"]: \"geometry\"})\n",
    "    gdf[\"Type\"] = details[\"Type\"]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "for k in NE_Data.keys():\n",
    "    v = NE_Data[k]\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_NE_Data(v)], ignore_index=True) )\n",
    "\n",
    "del NE_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historic England Data\n",
    " \n",
    "path = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\n",
    "HE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\n",
    "HE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\n",
    "HE_gdf[\"Type\"] = \"ConservationArea\"\n",
    "\n",
    "path = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\n",
    "LB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\n",
    "LB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\n",
    "LB_gdf[\"Type\"] = \"Listed Buildings\"\n",
    "\n",
    "HE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\n",
    "del LB_gdf\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\n",
    "del HE_gdf, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Met office data for 2020\n",
    "met_data = {\n",
    "    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\n",
    "    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\n",
    "    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\n",
    "    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\n",
    "    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\n",
    "    }\n",
    "\n",
    "def import_met(path, col, name):\n",
    "    pth = root_path +path\n",
    "    dnc = xr.open_dataset(pth)  \n",
    "    df = dnc.to_dataframe()\n",
    "\n",
    "    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\n",
    "    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\n",
    "    df = df.rename(columns={col: \"Details_Float\"})\n",
    "    df[\"Name\"] = name\n",
    "    df[\"Type\"] = \"MetOffice\"\n",
    "\n",
    "    #Convert to Geopandas dataframe\n",
    "    x_points = df[\"projection_x_coordinate\"].to_numpy()\n",
    "    y_points = df[\"projection_y_coordinate\"].to_numpy()\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "for k in met_data.keys():\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\n",
    "\n",
    "del met_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove invalid geometeries\n",
    "raw_gdf = raw_gdf[raw_gdf.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clip(name):\n",
    "    poly = kf.MakePolygon(KeyLocations[name], 5000, Main_CRS)\n",
    "    simple = gpd.clip(raw_gdf, poly)\n",
    "    simple.to_pickle(root_path + 'WorkingData/' + 'raw_gdf_' + name + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyLocations = {'Grasmere': [334692 , 506545],\n",
    "    'South_London' : [535493, 171250],\n",
    "    'Loch_Lomond' : [238450 , 690986],\n",
    "    'Furness' : [326877 , 474372],\n",
    "    'Croydon' : [532621 , 165313],\n",
    "    'StPauls' : [532052 , 181145],\n",
    "    'M25' : [503078 , 178200],\n",
    "    'M4_junct' : [519377 , 178360],\n",
    "    'M6_Junctions' : [357659 , 425462],\n",
    "    'Kendal': [353086 , 492882],\n",
    "    'Milford Haven': [190649 , 205747],\n",
    "    'Birmingham':[409295 , 290319],\n",
    "    'Euston':[529947 , 182697],\n",
    "    'Southampton':[441934 , 111013],\n",
    "    'Windermere': [341217 , 498029],\n",
    "    'Avonmouth': [351730 , 178243]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in KeyLocations.keys():\n",
    "    simple_clip(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RailwayStations',\n",
       " 'FerryTerminals',\n",
       " 'BusStops',\n",
       " 'RoadAccidents',\n",
       " 'Postcodes',\n",
       " 'LReg',\n",
       " 'FireStations',\n",
       " 'Schools',\n",
       " 'Crimes',\n",
       " 'StopAndSearch',\n",
       " 'Clinic',\n",
       " 'Dentists',\n",
       " 'GP',\n",
       " 'Hospital',\n",
       " 'Opticians',\n",
       " 'Pharmacy',\n",
       " 'Care homes and care at home',\n",
       " 'All_GB',\n",
       " 'National_Parks',\n",
       " 'LocalAuthorities',\n",
       " 'LSOA',\n",
       " 'GreenSpace',\n",
       " 'Rivers',\n",
       " 'Railway_Lines',\n",
       " 'Woodland_Region',\n",
       " 'Urban_Region',\n",
       " 'Foreshor_Region',\n",
       " 'Ferry_Line',\n",
       " 'Coastline',\n",
       " 'Lakes',\n",
       " 'Road',\n",
       " 'MotorwayJunction',\n",
       " 'Peat',\n",
       " 'SiteOfScientificInterest',\n",
       " 'LocalNatureReserve',\n",
       " 'NationalNatureReserve',\n",
       " 'SpecialAreaOfConservation',\n",
       " 'AncientWoodland',\n",
       " 'ConservationArea',\n",
       " 'Listed Buildings',\n",
       " 'MetOffice']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_gdf[\"Type\"].dropna().unique().tolist()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78a45328dcddc684fcc7e97834e2f9ed1c731cb5e93878b3fdd5359bd832b4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('GeoData': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
