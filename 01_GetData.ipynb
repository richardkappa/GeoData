{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/GeoData/\"\n",
    "Main_CRS = \"EPSG:27700\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into a standard structrue for the geopandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_obj(name ):\n",
    "    with open(root_path + 'WorkingData/' + name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Points(infile, inputs):\n",
    "    #print(inputs[\"Type\"])\n",
    "    #Get the before and after columns names\n",
    "    usecols = list(filter(None,list(inputs.values())[1:8]))\n",
    "    col_names = list({key: value for key, value in inputs.items() if value in usecols}.keys())\n",
    "\n",
    "    #Get the data types\n",
    "    t_dtypes = {key: value for key, value in dtypes.items() if key in col_names}\n",
    "    dtypes_in = {inputs[k]:v for k, v in t_dtypes.items()}\n",
    "    dtype_out = {key: value for key, value in dtypes.items() if key in col_names}\n",
    "\n",
    "    #Import the data\n",
    "    df = pd.read_csv(infile, usecols=usecols, encoding = \"ISO-8859-1\", dtype=dtypes_in) \n",
    "\n",
    "    #Reformat ready for next step\n",
    "    df.columns = col_names\n",
    "    df = df.astype(dtype_out)\n",
    "\n",
    "    df[\"Type\"] = inputs[\"Type\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data\n",
    "rail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\n",
    "rail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "ferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\n",
    "ferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "bus_infile = root_path + \"NaPTANcsv/Stops.csv\"\n",
    "bus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "acc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\n",
    "acc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\n",
    "\n",
    "NSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\n",
    "NSPL_inputs = {\"Type\":\"Postcodes\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\n",
    "\n",
    "#NSUL_Infile defined below\n",
    "NSUL_inputs = {\"Type\":\"UDPRNs\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\n",
    "\n",
    "point_pairs = [[rail_infile, rail_inputs],\n",
    "[ferry_infile, ferry_inputs],\n",
    "[bus_infile, bus_inputs],\n",
    "[acc2020_infile, acc2020_inputs],\n",
    "[NSPL_infile,NSPL_inputs]]\n",
    "\n",
    "NSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\n",
    "\n",
    "point_pairs = point_pairs + NSUL_Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate the standard structure\n",
    "Points_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\n",
    "dtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\n",
    "Points_of_Interest = Points_of_Interest.astype(dtypes)\n",
    "Points_of_Interest = Points_of_Interest.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in point_pairs:\n",
    "    Points_of_Interest = Points_of_Interest.append(Import_Points(l[0], l[1]), ignore_index=True)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"].astype(str).str.replace(\" \",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\n",
    "LReg_Names = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', \n",
    "              'Old_New', 'Duration', 'PAON', 'SAON', 'Street', 'Locality', 'Town_City', 'District', 'County', \n",
    "              'PPDCategory_Type', 'Record_Status_monthly_file_only']\n",
    "\n",
    "usecols = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', 'Old_New', 'Duration']\n",
    "\n",
    "LReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \n",
    "LReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\n",
    "\n",
    "LReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "LReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "LReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "LReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\n",
    "    LReg_Data[\"Property_Type\"].to_numpy(), \n",
    "    LReg_Data[\"Old_New\"].to_numpy(), \n",
    "    LReg_Data[\"Duration\"].to_numpy())\n",
    "    ).tolist()\n",
    "\n",
    "LReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\n",
    "LReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\n",
    "LReg_Data[\"Type\"] = \"LReg\"\n",
    "LReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\n",
    "del LReg_Data, LReg_Data2020, LReg_Data2021, LReg_Names, usecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firestation Data\n",
    "FireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \n",
    "                         engine=\"odf\",\n",
    "                        sheet_name = \"STATIONS\")\n",
    "\n",
    "FireStations[\"Type\"] = \"FireStations\"\n",
    "FireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\n",
    "FireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\n",
    "FireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\n",
    "FireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\n",
    "del FireStations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schools Data\n",
    "usecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\n",
    "Schools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Schools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "Schools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "Schools[\"Type\"] = \"Schools\"\n",
    "Schools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\n",
    "Schools[\"Name\"] = Schools[\"EstablishmentName\"]\n",
    "Schools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\n",
    "del Schools, usecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computer doesn't have enough ram to deal with UDPRN data so need to drop it here\n",
    "Points_of_Interest = Points_of_Interest[Points_of_Interest[\"Type\"]!=\"UDPRNs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We know that some of the easings and northings are on the Irish grid, split these out\n",
    "NI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"Postcodes\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"UDPRNs\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\n",
    "\n",
    "NI = Points_of_Interest.loc[NI_Mask,:]\n",
    "Points_of_Interest = Points_of_Interest.loc[~NI_Mask,:]\n",
    "\n",
    "del NI_Mask_1, NI_Mask_2, NI_Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Geopandas dataframe\n",
    "x_points = Points_of_Interest[\"Easting\"].to_numpy()\n",
    "y_points = Points_of_Interest[\"Northing\"].to_numpy()\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\n",
    "del Points_of_Interest, x_points, y_points\n",
    "\n",
    "#Do the same for NI, and convert to the appropriate crs\n",
    "x_points = NI[\"Easting\"].to_numpy()\n",
    "y_points = NI[\"Northing\"].to_numpy()\n",
    "\n",
    "NI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\n",
    "NI_gdf = NI_gdf.to_crs(Main_CRS)\n",
    "del NI, x_points, y_points\n",
    "\n",
    "#Re-combine now they are on the same coordinates\n",
    "raw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, NI_gdf], ignore_index=True) )\n",
    "del NI_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Police Crime data\n",
    "usecols = [\"Crime type\", \"Month\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64, \"Month\":\"str\"}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"street.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "Crimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\n",
    "\n",
    "for file in infile[1:]:\n",
    "    Crimes=Crimes.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\n",
    "\n",
    "Crimes = Crimes.dropna()\n",
    "Crimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\n",
    "Crimes = Crimes.rename(columns={\"Month\": \"Details_Str\"})\n",
    "Crimes[\"Type\"] = \"Crimes\"\n",
    "\n",
    "#Police Stop and Search Data\n",
    "usecols = [\"Object of search\", \"Date\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"search.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "StopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"])\n",
    "\n",
    "for file in file_list[1:]:\n",
    "    StopAndSearch=StopAndSearch.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"]), ignore_index=True)\n",
    "\n",
    "StopAndSearch = StopAndSearch.dropna()\n",
    "StopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\n",
    "StopAndSearch[\"Date\"] = StopAndSearch[\"Date\"].dt.strftime('%Y-%m-%d')\n",
    "StopAndSearch = StopAndSearch.rename(columns={\"Date\": \"Details_Str\"})\n",
    "StopAndSearch[\"Type\"] = \"StopAndSearch\"\n",
    "\n",
    "Crimes = Crimes.append(StopAndSearch,ignore_index=True)\n",
    "del StopAndSearch\n",
    "\n",
    "#Convert to Geopandas dataframe\n",
    "x_points = Crimes[\"Longitude\"].to_numpy()\n",
    "y_points = Crimes[\"Latitude\"].to_numpy()\n",
    "\n",
    "crimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\n",
    "crimes_gdf = crimes_gdf.to_crs(Main_CRS)\n",
    "crimes_gdf\n",
    "\n",
    "del Crimes\n",
    "\n",
    "#combine with the main points gdf\n",
    "raw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, crimes_gdf], ignore_index=True) )\n",
    "del crimes_gdf, file_list, infile, usecols, in_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NHS Data\n",
    "\n",
    "usecols = [\"OrganisationType\", \"OrganisationName\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"OrganisationType\":\"str\", \"OrganisationName\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "NHS_list = [root_path + \"NHS/\" + str(i) for i in os.listdir(\"D:/GeoData/NHS\") if \".csv\" in i]\n",
    "\n",
    "NHS = pd.read_csv(NHS_list[0], sep='¬'  , engine='python', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes)\n",
    "\n",
    "for file in NHS_list[1:]:\n",
    "    NHS = NHS.append(pd.read_csv(file, sep='¬'  , engine='python', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes), ignore_index=True)\n",
    "\n",
    "NHS = NHS.rename(columns={\"OrganisationType\":\"Type\", \"OrganisationName\":\"Name\"})\n",
    "\n",
    "#Convert to Geopandas dataframe\n",
    "x_points = NHS[\"Longitude\"].to_numpy()\n",
    "y_points = NHS[\"Latitude\"].to_numpy()\n",
    "\n",
    "NHS_gdf = gpd.GeoDataFrame(NHS.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\n",
    "NHS_gdf = NHS_gdf.to_crs(Main_CRS)\n",
    "NHS_gdf\n",
    "\n",
    "del NHS\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, NHS_gdf], ignore_index=True) )\n",
    "\n",
    "del NHS_gdf, usecols, in_dtypes, NHS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shape_Loc = {\n",
    "    'All_GB' : [root_path + \"Countries__December_2019__Boundaries_UK_BFC-shp/Countries__December_2019__Boundaries_UK_BFC.shp\", \"ctry19nm\"],\n",
    "    'National_Parks' : [root_path + \"National_Parks__December_2019__GB_BFE-shp/National_Parks__December_2019__GB_BFE.shp\", \"NPARK19NM\"],\n",
    "    'LocalAuthorities' : [root_path + \"Local_Authority_Districts__May_2020__Boundaries_UK_BFC-shp/Local_Authority_Districts__May_2020__Boundaries_UK_BFC.shp\", \"LAD20NM\"],\n",
    "    'LSOA' : [root_path + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2-shp/Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.shp\", \"LSOA11NMW\"],\n",
    "    'GreenSpace' : [root_path + \"opgrsp_essh_gb/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp\", \"id\"],\n",
    "    'Rivers' : [root_path + \"oprvrs_essh_gb/data/WatercourseLink.shp\",\"name1\"],\n",
    "    'Railway_Lines' : [root_path + \"strtgi_essh_gb/data/railway_line.shp\",\"LEGEND\"],\n",
    "    'Woodland_Region' : [root_path + \"strtgi_essh_gb/data/woodland_region.shp\",\"LEGEND\"],\n",
    "    'Urban_Region' : [root_path + \"strtgi_essh_gb/data/urban_region.shp\",\"LEGEND\"],\n",
    "    'Foreshor_Region' : [root_path + \"strtgi_essh_gb/data/foreshor_region.shp\",\"LEGEND\"],\n",
    "    'Ferry_Line' : [root_path + \"strtgi_essh_gb/data/ferry_line.shp\",\"LEGEND\", [\"FERRY_TIME\", \"FERRY_TYPE\", \"RESTRICTIO\", \"ACCESS\"]],\n",
    "    'Coastline' : [root_path + \"strtgi_essh_gb/data/coastline.shp\",\"LEGEND\"],\n",
    "    'Lakes' : [root_path + \"strtgi_essh_gb/data/lakes_region.shp\", \"LEGEND\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getextra(list):\n",
    "    try:\n",
    "        extra = list[2]\n",
    "        extraexists = True\n",
    "    except:\n",
    "        extra = []\n",
    "        extraexists = False\n",
    "\n",
    "    return extra, extraexists\n",
    "\n",
    "def get_shapefile(v):\n",
    "    extra, extraexists = getextra(v)\n",
    "    gdf = gpd.read_file(v[0]).to_crs(Main_CRS).loc[:,[v[1], \"geometry\"]+extra]\n",
    "    gdf = gdf.rename(columns={v[1]: \"Name\"})\n",
    "    gdf[\"Type\"] = k\n",
    "\n",
    "    usecols = [\"Type\", \"Name\", \"geometry\"]\n",
    "\n",
    "    if extraexists:\n",
    "        array = gdf[extra[0]].to_numpy()\n",
    "        if len(extra)>1:\n",
    "            for e in extra[1:]:\n",
    "                array = np.column_stack((array,gdf[e].to_numpy()))\n",
    "        gdf[\"Details_Str\"] = array.tolist()\n",
    "        usecols = usecols + [\"Details_Str\"]\n",
    "\n",
    "    gdf = gdf[usecols]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add all of the shape files to the master list\n",
    "\n",
    "for k in Shape_Loc.keys():\n",
    "    v = Shape_Loc[k]\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_shapefile(v)], ignore_index=True) )\n",
    "\n",
    "del Shape_Loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the roads\n",
    "file = os.listdir(\"D:\\GeoData\\oproad_essh_gb\\data\")\n",
    "\n",
    "path = [os.path.join(\"D:\\GeoData\\oproad_essh_gb\\data\", i) for i in file if \"RoadLink.shp\" in i]\n",
    "\n",
    "Roads = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path], ignore_index=True), crs=Main_CRS)\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, Roads], ignore_index=True) )\n",
    "del Roads, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the motorway junctions\n",
    "path = [os.path.join(\"D:\\GeoData\\oproad_essh_gb\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\n",
    "\n",
    "MotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\n",
    "del MotorwayJunctions, path, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural England data\n",
    "NE_Data = {\n",
    "    \"NE_Peat\" : {\"url\":\"https://opendata.arcgis.com/datasets/55b21c31a61c4292a465d7618d831eb8_0.geojson\", \"Type\":\"Peat\", \"Name\":\"PCLASSDESC\", \"Details_Str\":\"Type\", \"geometry\":\"geometry\"},\n",
    "    \"NE_SSI\" : {\"url\":\"https://opendata.arcgis.com/datasets/03fd7a2f8e4e4346bf41ce7879153949_0.geojson\", \"Type\":\"SiteOfScientificInterest\", \"Name\":\"SSSI_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_LocalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/b1d690ac6dd54c15bdd2d341b686ecd7_0.geojson\", \"Type\":\"LocalNatureReserve\", \"Name\":\"LNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_NationalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/ab7bfd86f5b347df8d47fc9bfab80caf_0.geojson\", \"Type\":\"NationalNatureReserve\", \"Name\":\"NNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\n",
    "    \"NE_SpecialAreaOfConvervation\" : {\"url\":\"https://opendata.arcgis.com/datasets/e4142658906c498fa37f0a20d3fdfcff_0.geojson\", \"Type\":\"SpecialAreaOfConservation\", \"Name\":\"SAC_NAME\", \"Details_Str\":\"SAC_CODE\", \"geometry\":\"geometry\"},\n",
    "    \"NE_AncientWoodland\" : {\"url\":\"https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson\", \"Type\":\"AncientWoodland\", \"Name\":\"NAME\", \"Details_Str\":\"THEMNAME\", \"geometry\":\"geometry\"}\n",
    "}\n",
    "\n",
    "def get_NE_Data(details):\n",
    "    url = details[\"url\"]\n",
    "\n",
    "    usecols = [details[\"Name\"], details[\"Details_Str\"], details[\"geometry\"]]\n",
    "\n",
    "    gdf = gpd.read_file(url).to_crs(Main_CRS).loc[:, usecols]\n",
    "\n",
    "\n",
    "    gdf = gdf.rename(columns={details[\"Name\"]: \"Name\"})\n",
    "    gdf = gdf.rename(columns={details[\"Details_Str\"]: \"Details_Str\"})\n",
    "    gdf = gdf.rename(columns={details[\"geometry\"]: \"geometry\"})\n",
    "    gdf[\"Type\"] = details[\"Type\"]\n",
    "\n",
    "    return gdf\n",
    "\n",
    "for k in NE_Data.keys():\n",
    "    v = NE_Data[k]\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_NE_Data(v)], ignore_index=True) )\n",
    "\n",
    "del NE_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historic England Data\n",
    " \n",
    "path = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\n",
    "HE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\n",
    "HE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\n",
    "HE_gdf[\"Type\"] = \"ConservationArea\"\n",
    "\n",
    "path = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\n",
    "LB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\n",
    "LB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\n",
    "LB_gdf[\"Type\"] = \"Listed Buildings\"\n",
    "\n",
    "HE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\n",
    "del LB_gdf\n",
    "\n",
    "raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\n",
    "del HE_gdf, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Met office data for 2020\n",
    "met_data = {\n",
    "    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\n",
    "    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\n",
    "    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\n",
    "    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\n",
    "    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\n",
    "    }\n",
    "\n",
    "def import_met(path, col, name):\n",
    "    pth = root_path +path\n",
    "    dnc = xr.open_dataset(pth)  \n",
    "    df = dnc.to_dataframe()\n",
    "\n",
    "    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\n",
    "    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\n",
    "    df = df.rename(columns={col: \"Details_Float\"})\n",
    "    df[\"Name\"] = name\n",
    "    df[\"Type\"] = \"MetOffice\"\n",
    "\n",
    "    #Convert to Geopandas dataframe\n",
    "    x_points = df[\"projection_x_coordinate\"].to_numpy()\n",
    "    y_points = df[\"projection_y_coordinate\"].to_numpy()\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "for k in met_data.keys():\n",
    "    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\n",
    "\n",
    "del met_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import pandas as pd\\nimport numpy as np\\nimport os\\nimport glob\\nimport geopandas as gpd\\nimport xarray as xr',\n",
       "  'root_path = \"D:/GeoData/\"\\nMain_CRS = \"EPSG:27700\"',\n",
       "  \"import pickle\\ndef load_obj(name ):\\n    with open(root_path + 'WorkingData/' + name + '.pickle', 'rb') as f:\\n        return pickle.load(f)\",\n",
       "  'def Import_Points(infile, inputs):\\n    #print(inputs[\"Type\"])\\n    #Get the before and after columns names\\n    usecols = list(filter(None,list(inputs.values())[1:8]))\\n    col_names = list({key: value for key, value in inputs.items() if value in usecols}.keys())\\n\\n    #Get the data types\\n    t_dtypes = {key: value for key, value in dtypes.items() if key in col_names}\\n    dtypes_in = {inputs[k]:v for k, v in t_dtypes.items()}\\n    dtype_out = {key: value for key, value in dtypes.items() if key in col_names}\\n\\n    #Import the data\\n    df = pd.read_csv(infile, usecols=usecols, encoding = \"ISO-8859-1\", dtype=dtypes_in) \\n\\n    #Reformat ready for next step\\n    df.columns = col_names\\n    df = df.astype(dtype_out)\\n\\n    df[\"Type\"] = inputs[\"Type\"]\\n    return df',\n",
       "  '#source data\\nrail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\\nrail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\\nferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nbus_infile = root_path + \"NaPTANcsv/Stops.csv\"\\nbus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nacc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\\nacc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\\n\\nNSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\\nNSPL_inputs = {\"Type\":\"Postcodes\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\\n\\n#NSUL_Infile defined below\\nNSUL_inputs = {\"Type\":\"UDPRNs\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\\n\\npoint_pairs = [[rail_infile, rail_inputs],\\n[ferry_infile, ferry_inputs],\\n[bus_infile, bus_inputs],\\n[acc2020_infile, acc2020_inputs],\\n[NSPL_infile,NSPL_inputs]]\\n\\nNSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\\n\\npoint_pairs = point_pairs + NSUL_Pairs',\n",
       "  '#initiate the standard structure\\nPoints_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\\ndtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\\nPoints_of_Interest = Points_of_Interest.astype(dtypes)\\nPoints_of_Interest = Points_of_Interest.drop(0)',\n",
       "  'for l in point_pairs:\\n    Points_of_Interest = Points_of_Interest.append(Import_Points(l[0], l[1]), ignore_index=True)\\n\\n#remove the spaces from the postcode\\nPoints_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"].astype(str).str.replace(\" \",\"\")',\n",
       "  '#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\\nLReg_Names = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\n              \\'Old_New\\', \\'Duration\\', \\'PAON\\', \\'SAON\\', \\'Street\\', \\'Locality\\', \\'Town_City\\', \\'District\\', \\'County\\', \\n              \\'PPDCategory_Type\\', \\'Record_Status_monthly_file_only\\']\\n\\nusecols = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\'Old_New\\', \\'Duration\\']\\n\\nLReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \\nLReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\\n\\nLReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\\n\\n#remove the spaces from the postcode\\nLReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nLReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nLReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\\n    LReg_Data[\"Property_Type\"].to_numpy(), \\n    LReg_Data[\"Old_New\"].to_numpy(), \\n    LReg_Data[\"Duration\"].to_numpy())\\n    ).tolist()\\n\\nLReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\\nLReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\\nLReg_Data[\"Type\"] = \"LReg\"\\nLReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\\ndel LReg_Data, LReg_Data2020, LReg_Data2021, LReg_Names, usecols',\n",
       "  '#Firestation Data\\nFireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \\n                         engine=\"odf\",\\n                        sheet_name = \"STATIONS\")\\n\\nFireStations[\"Type\"] = \"FireStations\"\\nFireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\\nFireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\\nFireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\\nFireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\\ndel FireStations',\n",
       "  '#Schools Data\\nusecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\\nSchools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\\n\\n#remove the spaces from the postcode\\nSchools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nSchools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nSchools[\"Type\"] = \"Schools\"\\nSchools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\\nSchools[\"Name\"] = Schools[\"EstablishmentName\"]\\nSchools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\\ndel Schools, usecols',\n",
       "  '#Computer doesn\\'t have enough ram to deal with UDPRN data so need to drop it here\\nPoints_of_Interest = Points_of_Interest[Points_of_Interest[\"Type\"]!=\"UDPRNs\"]',\n",
       "  '#We know that some of the easings and northings are on the Irish grid, split these out\\nNI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"Postcodes\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"UDPRNs\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\\n\\nNI = Points_of_Interest.loc[NI_Mask,:]\\nPoints_of_Interest = Points_of_Interest.loc[~NI_Mask,:]\\n\\ndel NI_Mask_1, NI_Mask_2, NI_Mask',\n",
       "  '#Convert to Geopandas dataframe\\nx_points = Points_of_Interest[\"Easting\"].to_numpy()\\ny_points = Points_of_Interest[\"Northing\"].to_numpy()\\n\\nraw_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\ndel Points_of_Interest, x_points, y_points\\n\\n#Do the same for NI, and convert to the appropriate crs\\nx_points = NI[\"Easting\"].to_numpy()\\ny_points = NI[\"Northing\"].to_numpy()\\n\\nNI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\\nNI_gdf = NI_gdf.to_crs(Main_CRS)\\ndel NI, x_points, y_points\\n\\n#Re-combine now they are on the same coordinates\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, NI_gdf], ignore_index=True) )\\ndel NI_gdf',\n",
       "  '#Police Crime data\\nusecols = [\"Crime type\", \"Month\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64, \"Month\":\"str\"}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"street.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nCrimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\\n\\nfor file in infile[1:]:\\n    Crimes=Crimes.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\\n\\nCrimes = Crimes.dropna()\\nCrimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\\nCrimes = Crimes.rename(columns={\"Month\": \"Details_Str\"})\\nCrimes[\"Type\"] = \"Crimes\"\\n\\n#Police Stop and Search Data\\nusecols = [\"Object of search\", \"Date\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"search.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nStopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"])\\n\\nfor file in file_list[1:]:\\n    StopAndSearch=StopAndSearch.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"]), ignore_index=True)\\n\\nStopAndSearch = StopAndSearch.dropna()\\nStopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\\nStopAndSearch[\"Date\"] = StopAndSearch[\"Date\"].dt.strftime(\\'%Y-%m-%d\\')\\nStopAndSearch = StopAndSearch.rename(columns={\"Date\": \"Details_Str\"})\\nStopAndSearch[\"Type\"] = \"StopAndSearch\"\\n\\nCrimes = Crimes.append(StopAndSearch,ignore_index=True)\\ndel StopAndSearch\\n\\n#Convert to Geopandas dataframe\\nx_points = Crimes[\"Longitude\"].to_numpy()\\ny_points = Crimes[\"Latitude\"].to_numpy()\\n\\ncrimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\ncrimes_gdf = crimes_gdf.to_crs(Main_CRS)\\ncrimes_gdf\\n\\ndel Crimes\\n\\n#combine with the main points gdf\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, crimes_gdf], ignore_index=True) )\\ndel crimes_gdf, file_list, infile, usecols, in_dtypes',\n",
       "  '#NHS Data\\n\\nusecols = [\"OrganisationType\", \"OrganisationName\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"OrganisationType\":\"str\", \"OrganisationName\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nNHS_list = [root_path + \"NHS/\" + str(i) for i in os.listdir(\"D:/GeoData/NHS\") if \".csv\" in i]\\n\\nNHS = pd.read_csv(NHS_list[0], sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes)\\n\\nfor file in NHS_list[1:]:\\n    NHS = NHS.append(pd.read_csv(file, sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes), ignore_index=True)\\n\\nNHS = NHS.rename(columns={\"OrganisationType\":\"Type\", \"OrganisationName\":\"Name\"})\\n\\n#Convert to Geopandas dataframe\\nx_points = NHS[\"Longitude\"].to_numpy()\\ny_points = NHS[\"Latitude\"].to_numpy()\\n\\nNHS_gdf = gpd.GeoDataFrame(NHS.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\nNHS_gdf = NHS_gdf.to_crs(Main_CRS)\\nNHS_gdf\\n\\ndel NHS\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, NHS_gdf], ignore_index=True) )\\n\\ndel NHS_gdf, usecols, in_dtypes, NHS_list',\n",
       "  'Shape_Loc = {\\n    \\'All_GB\\' : [root_path + \"Countries__December_2019__Boundaries_UK_BFC-shp/Countries__December_2019__Boundaries_UK_BFC.shp\", \"ctry19nm\"],\\n    \\'National_Parks\\' : [root_path + \"National_Parks__December_2019__GB_BFE-shp/National_Parks__December_2019__GB_BFE.shp\", \"NPARK19NM\"],\\n    \\'LocalAuthorities\\' : [root_path + \"Local_Authority_Districts__May_2020__Boundaries_UK_BFC-shp/Local_Authority_Districts__May_2020__Boundaries_UK_BFC.shp\", \"LAD20NM\"],\\n    \\'LSOA\\' : [root_path + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2-shp/Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.shp\", \"LSOA11NMW\"],\\n    \\'GreenSpace\\' : [root_path + \"opgrsp_essh_gb/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp\", \"id\"],\\n    \\'Rivers\\' : [root_path + \"oprvrs_essh_gb/data/WatercourseLink.shp\",\"name1\"],\\n    \\'Railway_Lines\\' : [root_path + \"strtgi_essh_gb/data/railway_line.shp\",\"LEGEND\"],\\n    \\'Woodland_Region\\' : [root_path + \"strtgi_essh_gb/data/woodland_region.shp\",\"LEGEND\"],\\n    \\'Urban_Region\\' : [root_path + \"strtgi_essh_gb/data/urban_region.shp\",\"LEGEND\"],\\n    \\'Foreshor_Region\\' : [root_path + \"strtgi_essh_gb/data/foreshor_region.shp\",\"LEGEND\"],\\n    \\'Ferry_Line\\' : [root_path + \"strtgi_essh_gb/data/ferry_line.shp\",\"LEGEND\", [\"FERRY_TIME\", \"FERRY_TYPE\", \"RESTRICTIO\", \"ACCESS\"]],\\n    \\'Coastline\\' : [root_path + \"strtgi_essh_gb/data/coastline.shp\",\"LEGEND\"],\\n    \\'Lakes\\' : [root_path + \"strtgi_essh_gb/data/lakes_region.shp\", \"LEGEND\"]}',\n",
       "  'def getextra(list):\\n    try:\\n        extra = list[2]\\n        extraexists = True\\n    except:\\n        extra = []\\n        extraexists = False\\n\\n    return extra, extraexists\\n\\ndef get_shapefile(v):\\n    extra, extraexists = getextra(v)\\n    gdf = gpd.read_file(v[0]).to_crs(Main_CRS).loc[:,[v[1], \"geometry\"]+extra]\\n    gdf = gdf.rename(columns={v[1]: \"Name\"})\\n    gdf[\"Type\"] = k\\n\\n    usecols = [\"Type\", \"Name\", \"geometry\"]\\n\\n    if extraexists:\\n        array = gdf[extra[0]].to_numpy()\\n        if len(extra)>1:\\n            for e in extra[1:]:\\n                array = np.column_stack((array,gdf[e].to_numpy()))\\n        gdf[\"Details_Str\"] = array.tolist()\\n        usecols = usecols + [\"Details_Str\"]\\n\\n    gdf = gdf[usecols]\\n\\n    return gdf',\n",
       "  '#Add all of the shape files to the master list\\n\\nfor k in Shape_Loc.keys():\\n    v = Shape_Loc[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_shapefile(v)], ignore_index=True) )\\n\\ndel Shape_Loc',\n",
       "  '#add the roads\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\n\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"RoadLink.shp\" in i]\\n\\nRoads = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path], ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, Roads], ignore_index=True) )\\ndel Roads, file, path',\n",
       "  '#Add the motorway junctions\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       "  '#Add the motorway junctions\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       "  '#Natural England data\\nNE_Data = {\\n    \"NE_Peat\" : {\"url\":\"https://opendata.arcgis.com/datasets/55b21c31a61c4292a465d7618d831eb8_0.geojson\", \"Type\":\"Peat\", \"Name\":\"PCLASSDESC\", \"Details_Str\":\"Type\", \"geometry\":\"geometry\"},\\n    \"NE_SSI\" : {\"url\":\"https://opendata.arcgis.com/datasets/03fd7a2f8e4e4346bf41ce7879153949_0.geojson\", \"Type\":\"SiteOfScientificInterest\", \"Name\":\"SSSI_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_LocalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/b1d690ac6dd54c15bdd2d341b686ecd7_0.geojson\", \"Type\":\"LocalNatureReserve\", \"Name\":\"LNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_NationalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/ab7bfd86f5b347df8d47fc9bfab80caf_0.geojson\", \"Type\":\"NationalNatureReserve\", \"Name\":\"NNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_SpecialAreaOfConvervation\" : {\"url\":\"https://opendata.arcgis.com/datasets/e4142658906c498fa37f0a20d3fdfcff_0.geojson\", \"Type\":\"SpecialAreaOfConservation\", \"Name\":\"SAC_NAME\", \"Details_Str\":\"SAC_CODE\", \"geometry\":\"geometry\"},\\n    \"NE_AncientWoodland\" : {\"url\":\"https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson\", \"Type\":\"AncientWoodland\", \"Name\":\"NAME\", \"Details_Str\":\"THEMNAME\", \"geometry\":\"geometry\"}\\n}\\n\\ndef get_NE_Data(details):\\n    url = details[\"url\"]\\n\\n    usecols = [details[\"Name\"], details[\"Details_Str\"], details[\"geometry\"]]\\n\\n    gdf = gpd.read_file(url).to_crs(Main_CRS).loc[:, usecols]\\n\\n\\n    gdf = gdf.rename(columns={details[\"Name\"]: \"Name\"})\\n    gdf = gdf.rename(columns={details[\"Details_Str\"]: \"Details_Str\"})\\n    gdf = gdf.rename(columns={details[\"geometry\"]: \"geometry\"})\\n    gdf[\"Type\"] = details[\"Type\"]\\n\\n    return gdf\\n\\nfor k in NE_Data.keys():\\n    v = NE_Data[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_NE_Data(v)], ignore_index=True) )\\n\\ndel NE_Data',\n",
       "  '#Historic England Data\\n \\npath = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\\nHE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\\nHE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\\nHE_gdf[\"Type\"] = \"ConservationArea\"\\n\\npath = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\\nLB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\\nLB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\\nLB_gdf[\"Type\"] = \"Listed Buildings\"\\n\\nHE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\\ndel LB_gdf\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\\ndel HE_gdf, path',\n",
       "  '#Met office data for 2020\\nmet_data = {\\n    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\\n    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\\n    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\\n    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\\n    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\\n    }\\n\\ndef import_met(path, col, name):\\n    pth = root_path +path\\n    dnc = xr.open_dataset(pth)  \\n    df = dnc.to_dataframe()\\n\\n    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\\n    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\\n    df = df.rename(columns={col: \"Details_Float\"})\\n    df[\"Name\"] = name\\n    df[\"Type\"] = \"MetOffice\"\\n\\n    #Convert to Geopandas dataframe\\n    x_points = df[\"projection_x_coordinate\"].to_numpy()\\n    y_points = df[\"projection_y_coordinate\"].to_numpy()\\n\\n    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\n\\n    return gdf\\n\\nfor k in met_data.keys():\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\\n\\ndel met_data',\n",
       "  \"raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pickle')\",\n",
       "  'locals()'],\n",
       " '_oh': {},\n",
       " '_dh': ['d:\\\\G_Richard\\\\Documents\\\\Coding\\\\GeoData'],\n",
       " 'In': ['',\n",
       "  'import pandas as pd\\nimport numpy as np\\nimport os\\nimport glob\\nimport geopandas as gpd\\nimport xarray as xr',\n",
       "  'root_path = \"D:/GeoData/\"\\nMain_CRS = \"EPSG:27700\"',\n",
       "  \"import pickle\\ndef load_obj(name ):\\n    with open(root_path + 'WorkingData/' + name + '.pickle', 'rb') as f:\\n        return pickle.load(f)\",\n",
       "  'def Import_Points(infile, inputs):\\n    #print(inputs[\"Type\"])\\n    #Get the before and after columns names\\n    usecols = list(filter(None,list(inputs.values())[1:8]))\\n    col_names = list({key: value for key, value in inputs.items() if value in usecols}.keys())\\n\\n    #Get the data types\\n    t_dtypes = {key: value for key, value in dtypes.items() if key in col_names}\\n    dtypes_in = {inputs[k]:v for k, v in t_dtypes.items()}\\n    dtype_out = {key: value for key, value in dtypes.items() if key in col_names}\\n\\n    #Import the data\\n    df = pd.read_csv(infile, usecols=usecols, encoding = \"ISO-8859-1\", dtype=dtypes_in) \\n\\n    #Reformat ready for next step\\n    df.columns = col_names\\n    df = df.astype(dtype_out)\\n\\n    df[\"Type\"] = inputs[\"Type\"]\\n    return df',\n",
       "  '#source data\\nrail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\\nrail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\\nferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nbus_infile = root_path + \"NaPTANcsv/Stops.csv\"\\nbus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nacc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\\nacc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\\n\\nNSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\\nNSPL_inputs = {\"Type\":\"Postcodes\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\\n\\n#NSUL_Infile defined below\\nNSUL_inputs = {\"Type\":\"UDPRNs\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\\n\\npoint_pairs = [[rail_infile, rail_inputs],\\n[ferry_infile, ferry_inputs],\\n[bus_infile, bus_inputs],\\n[acc2020_infile, acc2020_inputs],\\n[NSPL_infile,NSPL_inputs]]\\n\\nNSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\\n\\npoint_pairs = point_pairs + NSUL_Pairs',\n",
       "  '#initiate the standard structure\\nPoints_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\\ndtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\\nPoints_of_Interest = Points_of_Interest.astype(dtypes)\\nPoints_of_Interest = Points_of_Interest.drop(0)',\n",
       "  'for l in point_pairs:\\n    Points_of_Interest = Points_of_Interest.append(Import_Points(l[0], l[1]), ignore_index=True)\\n\\n#remove the spaces from the postcode\\nPoints_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"].astype(str).str.replace(\" \",\"\")',\n",
       "  '#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\\nLReg_Names = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\n              \\'Old_New\\', \\'Duration\\', \\'PAON\\', \\'SAON\\', \\'Street\\', \\'Locality\\', \\'Town_City\\', \\'District\\', \\'County\\', \\n              \\'PPDCategory_Type\\', \\'Record_Status_monthly_file_only\\']\\n\\nusecols = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\'Old_New\\', \\'Duration\\']\\n\\nLReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \\nLReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\\n\\nLReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\\n\\n#remove the spaces from the postcode\\nLReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nLReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nLReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\\n    LReg_Data[\"Property_Type\"].to_numpy(), \\n    LReg_Data[\"Old_New\"].to_numpy(), \\n    LReg_Data[\"Duration\"].to_numpy())\\n    ).tolist()\\n\\nLReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\\nLReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\\nLReg_Data[\"Type\"] = \"LReg\"\\nLReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\\ndel LReg_Data, LReg_Data2020, LReg_Data2021, LReg_Names, usecols',\n",
       "  '#Firestation Data\\nFireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \\n                         engine=\"odf\",\\n                        sheet_name = \"STATIONS\")\\n\\nFireStations[\"Type\"] = \"FireStations\"\\nFireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\\nFireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\\nFireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\\nFireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\\ndel FireStations',\n",
       "  '#Schools Data\\nusecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\\nSchools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\\n\\n#remove the spaces from the postcode\\nSchools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nSchools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nSchools[\"Type\"] = \"Schools\"\\nSchools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\\nSchools[\"Name\"] = Schools[\"EstablishmentName\"]\\nSchools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\\ndel Schools, usecols',\n",
       "  '#Computer doesn\\'t have enough ram to deal with UDPRN data so need to drop it here\\nPoints_of_Interest = Points_of_Interest[Points_of_Interest[\"Type\"]!=\"UDPRNs\"]',\n",
       "  '#We know that some of the easings and northings are on the Irish grid, split these out\\nNI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"Postcodes\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"UDPRNs\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\\n\\nNI = Points_of_Interest.loc[NI_Mask,:]\\nPoints_of_Interest = Points_of_Interest.loc[~NI_Mask,:]\\n\\ndel NI_Mask_1, NI_Mask_2, NI_Mask',\n",
       "  '#Convert to Geopandas dataframe\\nx_points = Points_of_Interest[\"Easting\"].to_numpy()\\ny_points = Points_of_Interest[\"Northing\"].to_numpy()\\n\\nraw_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\ndel Points_of_Interest, x_points, y_points\\n\\n#Do the same for NI, and convert to the appropriate crs\\nx_points = NI[\"Easting\"].to_numpy()\\ny_points = NI[\"Northing\"].to_numpy()\\n\\nNI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\\nNI_gdf = NI_gdf.to_crs(Main_CRS)\\ndel NI, x_points, y_points\\n\\n#Re-combine now they are on the same coordinates\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, NI_gdf], ignore_index=True) )\\ndel NI_gdf',\n",
       "  '#Police Crime data\\nusecols = [\"Crime type\", \"Month\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64, \"Month\":\"str\"}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"street.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nCrimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\\n\\nfor file in infile[1:]:\\n    Crimes=Crimes.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\\n\\nCrimes = Crimes.dropna()\\nCrimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\\nCrimes = Crimes.rename(columns={\"Month\": \"Details_Str\"})\\nCrimes[\"Type\"] = \"Crimes\"\\n\\n#Police Stop and Search Data\\nusecols = [\"Object of search\", \"Date\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"search.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nStopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"])\\n\\nfor file in file_list[1:]:\\n    StopAndSearch=StopAndSearch.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"]), ignore_index=True)\\n\\nStopAndSearch = StopAndSearch.dropna()\\nStopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\\nStopAndSearch[\"Date\"] = StopAndSearch[\"Date\"].dt.strftime(\\'%Y-%m-%d\\')\\nStopAndSearch = StopAndSearch.rename(columns={\"Date\": \"Details_Str\"})\\nStopAndSearch[\"Type\"] = \"StopAndSearch\"\\n\\nCrimes = Crimes.append(StopAndSearch,ignore_index=True)\\ndel StopAndSearch\\n\\n#Convert to Geopandas dataframe\\nx_points = Crimes[\"Longitude\"].to_numpy()\\ny_points = Crimes[\"Latitude\"].to_numpy()\\n\\ncrimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\ncrimes_gdf = crimes_gdf.to_crs(Main_CRS)\\ncrimes_gdf\\n\\ndel Crimes\\n\\n#combine with the main points gdf\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, crimes_gdf], ignore_index=True) )\\ndel crimes_gdf, file_list, infile, usecols, in_dtypes',\n",
       "  '#NHS Data\\n\\nusecols = [\"OrganisationType\", \"OrganisationName\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"OrganisationType\":\"str\", \"OrganisationName\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nNHS_list = [root_path + \"NHS/\" + str(i) for i in os.listdir(\"D:/GeoData/NHS\") if \".csv\" in i]\\n\\nNHS = pd.read_csv(NHS_list[0], sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes)\\n\\nfor file in NHS_list[1:]:\\n    NHS = NHS.append(pd.read_csv(file, sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes), ignore_index=True)\\n\\nNHS = NHS.rename(columns={\"OrganisationType\":\"Type\", \"OrganisationName\":\"Name\"})\\n\\n#Convert to Geopandas dataframe\\nx_points = NHS[\"Longitude\"].to_numpy()\\ny_points = NHS[\"Latitude\"].to_numpy()\\n\\nNHS_gdf = gpd.GeoDataFrame(NHS.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\nNHS_gdf = NHS_gdf.to_crs(Main_CRS)\\nNHS_gdf\\n\\ndel NHS\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, NHS_gdf], ignore_index=True) )\\n\\ndel NHS_gdf, usecols, in_dtypes, NHS_list',\n",
       "  'Shape_Loc = {\\n    \\'All_GB\\' : [root_path + \"Countries__December_2019__Boundaries_UK_BFC-shp/Countries__December_2019__Boundaries_UK_BFC.shp\", \"ctry19nm\"],\\n    \\'National_Parks\\' : [root_path + \"National_Parks__December_2019__GB_BFE-shp/National_Parks__December_2019__GB_BFE.shp\", \"NPARK19NM\"],\\n    \\'LocalAuthorities\\' : [root_path + \"Local_Authority_Districts__May_2020__Boundaries_UK_BFC-shp/Local_Authority_Districts__May_2020__Boundaries_UK_BFC.shp\", \"LAD20NM\"],\\n    \\'LSOA\\' : [root_path + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2-shp/Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.shp\", \"LSOA11NMW\"],\\n    \\'GreenSpace\\' : [root_path + \"opgrsp_essh_gb/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp\", \"id\"],\\n    \\'Rivers\\' : [root_path + \"oprvrs_essh_gb/data/WatercourseLink.shp\",\"name1\"],\\n    \\'Railway_Lines\\' : [root_path + \"strtgi_essh_gb/data/railway_line.shp\",\"LEGEND\"],\\n    \\'Woodland_Region\\' : [root_path + \"strtgi_essh_gb/data/woodland_region.shp\",\"LEGEND\"],\\n    \\'Urban_Region\\' : [root_path + \"strtgi_essh_gb/data/urban_region.shp\",\"LEGEND\"],\\n    \\'Foreshor_Region\\' : [root_path + \"strtgi_essh_gb/data/foreshor_region.shp\",\"LEGEND\"],\\n    \\'Ferry_Line\\' : [root_path + \"strtgi_essh_gb/data/ferry_line.shp\",\"LEGEND\", [\"FERRY_TIME\", \"FERRY_TYPE\", \"RESTRICTIO\", \"ACCESS\"]],\\n    \\'Coastline\\' : [root_path + \"strtgi_essh_gb/data/coastline.shp\",\"LEGEND\"],\\n    \\'Lakes\\' : [root_path + \"strtgi_essh_gb/data/lakes_region.shp\", \"LEGEND\"]}',\n",
       "  'def getextra(list):\\n    try:\\n        extra = list[2]\\n        extraexists = True\\n    except:\\n        extra = []\\n        extraexists = False\\n\\n    return extra, extraexists\\n\\ndef get_shapefile(v):\\n    extra, extraexists = getextra(v)\\n    gdf = gpd.read_file(v[0]).to_crs(Main_CRS).loc[:,[v[1], \"geometry\"]+extra]\\n    gdf = gdf.rename(columns={v[1]: \"Name\"})\\n    gdf[\"Type\"] = k\\n\\n    usecols = [\"Type\", \"Name\", \"geometry\"]\\n\\n    if extraexists:\\n        array = gdf[extra[0]].to_numpy()\\n        if len(extra)>1:\\n            for e in extra[1:]:\\n                array = np.column_stack((array,gdf[e].to_numpy()))\\n        gdf[\"Details_Str\"] = array.tolist()\\n        usecols = usecols + [\"Details_Str\"]\\n\\n    gdf = gdf[usecols]\\n\\n    return gdf',\n",
       "  '#Add all of the shape files to the master list\\n\\nfor k in Shape_Loc.keys():\\n    v = Shape_Loc[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_shapefile(v)], ignore_index=True) )\\n\\ndel Shape_Loc',\n",
       "  '#add the roads\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\n\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"RoadLink.shp\" in i]\\n\\nRoads = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path], ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, Roads], ignore_index=True) )\\ndel Roads, file, path',\n",
       "  '#Add the motorway junctions\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       "  '#Add the motorway junctions\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       "  '#Natural England data\\nNE_Data = {\\n    \"NE_Peat\" : {\"url\":\"https://opendata.arcgis.com/datasets/55b21c31a61c4292a465d7618d831eb8_0.geojson\", \"Type\":\"Peat\", \"Name\":\"PCLASSDESC\", \"Details_Str\":\"Type\", \"geometry\":\"geometry\"},\\n    \"NE_SSI\" : {\"url\":\"https://opendata.arcgis.com/datasets/03fd7a2f8e4e4346bf41ce7879153949_0.geojson\", \"Type\":\"SiteOfScientificInterest\", \"Name\":\"SSSI_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_LocalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/b1d690ac6dd54c15bdd2d341b686ecd7_0.geojson\", \"Type\":\"LocalNatureReserve\", \"Name\":\"LNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_NationalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/ab7bfd86f5b347df8d47fc9bfab80caf_0.geojson\", \"Type\":\"NationalNatureReserve\", \"Name\":\"NNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_SpecialAreaOfConvervation\" : {\"url\":\"https://opendata.arcgis.com/datasets/e4142658906c498fa37f0a20d3fdfcff_0.geojson\", \"Type\":\"SpecialAreaOfConservation\", \"Name\":\"SAC_NAME\", \"Details_Str\":\"SAC_CODE\", \"geometry\":\"geometry\"},\\n    \"NE_AncientWoodland\" : {\"url\":\"https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson\", \"Type\":\"AncientWoodland\", \"Name\":\"NAME\", \"Details_Str\":\"THEMNAME\", \"geometry\":\"geometry\"}\\n}\\n\\ndef get_NE_Data(details):\\n    url = details[\"url\"]\\n\\n    usecols = [details[\"Name\"], details[\"Details_Str\"], details[\"geometry\"]]\\n\\n    gdf = gpd.read_file(url).to_crs(Main_CRS).loc[:, usecols]\\n\\n\\n    gdf = gdf.rename(columns={details[\"Name\"]: \"Name\"})\\n    gdf = gdf.rename(columns={details[\"Details_Str\"]: \"Details_Str\"})\\n    gdf = gdf.rename(columns={details[\"geometry\"]: \"geometry\"})\\n    gdf[\"Type\"] = details[\"Type\"]\\n\\n    return gdf\\n\\nfor k in NE_Data.keys():\\n    v = NE_Data[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_NE_Data(v)], ignore_index=True) )\\n\\ndel NE_Data',\n",
       "  '#Historic England Data\\n \\npath = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\\nHE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\\nHE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\\nHE_gdf[\"Type\"] = \"ConservationArea\"\\n\\npath = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\\nLB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\\nLB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\\nLB_gdf[\"Type\"] = \"Listed Buildings\"\\n\\nHE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\\ndel LB_gdf\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\\ndel HE_gdf, path',\n",
       "  '#Met office data for 2020\\nmet_data = {\\n    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\\n    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\\n    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\\n    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\\n    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\\n    }\\n\\ndef import_met(path, col, name):\\n    pth = root_path +path\\n    dnc = xr.open_dataset(pth)  \\n    df = dnc.to_dataframe()\\n\\n    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\\n    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\\n    df = df.rename(columns={col: \"Details_Float\"})\\n    df[\"Name\"] = name\\n    df[\"Type\"] = \"MetOffice\"\\n\\n    #Convert to Geopandas dataframe\\n    x_points = df[\"projection_x_coordinate\"].to_numpy()\\n    y_points = df[\"projection_y_coordinate\"].to_numpy()\\n\\n    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\n\\n    return gdf\\n\\nfor k in met_data.keys():\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\\n\\ndel met_data',\n",
       "  \"raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pickle')\",\n",
       "  'locals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000025828762CA0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x25828790a60>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x25828790a60>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " 'os': <module 'os' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\os.py'>,\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " '_i': \"raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pickle')\",\n",
       " '_ii': '#Met office data for 2020\\nmet_data = {\\n    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\\n    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\\n    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\\n    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\\n    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\\n    }\\n\\ndef import_met(path, col, name):\\n    pth = root_path +path\\n    dnc = xr.open_dataset(pth)  \\n    df = dnc.to_dataframe()\\n\\n    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\\n    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\\n    df = df.rename(columns={col: \"Details_Float\"})\\n    df[\"Name\"] = name\\n    df[\"Type\"] = \"MetOffice\"\\n\\n    #Convert to Geopandas dataframe\\n    x_points = df[\"projection_x_coordinate\"].to_numpy()\\n    y_points = df[\"projection_y_coordinate\"].to_numpy()\\n\\n    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\n\\n    return gdf\\n\\nfor k in met_data.keys():\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\\n\\ndel met_data',\n",
       " '_iii': '#Historic England Data\\n \\npath = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\\nHE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\\nHE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\\nHE_gdf[\"Type\"] = \"ConservationArea\"\\n\\npath = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\\nLB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\\nLB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\\nLB_gdf[\"Type\"] = \"Listed Buildings\"\\n\\nHE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\\ndel LB_gdf\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\\ndel HE_gdf, path',\n",
       " '_i1': 'import pandas as pd\\nimport numpy as np\\nimport os\\nimport glob\\nimport geopandas as gpd\\nimport xarray as xr',\n",
       " 'pd': <module 'pandas' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>,\n",
       " 'np': <module 'numpy' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py'>,\n",
       " 'glob': <module 'glob' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\glob.py'>,\n",
       " 'gpd': <module 'geopandas' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\site-packages\\\\geopandas\\\\__init__.py'>,\n",
       " 'xr': <module 'xarray' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\site-packages\\\\xarray\\\\__init__.py'>,\n",
       " '_i2': 'root_path = \"D:/GeoData/\"\\nMain_CRS = \"EPSG:27700\"',\n",
       " 'root_path': 'D:/GeoData/',\n",
       " 'Main_CRS': 'EPSG:27700',\n",
       " '_i3': \"import pickle\\ndef load_obj(name ):\\n    with open(root_path + 'WorkingData/' + name + '.pickle', 'rb') as f:\\n        return pickle.load(f)\",\n",
       " 'pickle': <module 'pickle' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\pickle.py'>,\n",
       " 'load_obj': <function __main__.load_obj(name)>,\n",
       " '_i4': 'def Import_Points(infile, inputs):\\n    #print(inputs[\"Type\"])\\n    #Get the before and after columns names\\n    usecols = list(filter(None,list(inputs.values())[1:8]))\\n    col_names = list({key: value for key, value in inputs.items() if value in usecols}.keys())\\n\\n    #Get the data types\\n    t_dtypes = {key: value for key, value in dtypes.items() if key in col_names}\\n    dtypes_in = {inputs[k]:v for k, v in t_dtypes.items()}\\n    dtype_out = {key: value for key, value in dtypes.items() if key in col_names}\\n\\n    #Import the data\\n    df = pd.read_csv(infile, usecols=usecols, encoding = \"ISO-8859-1\", dtype=dtypes_in) \\n\\n    #Reformat ready for next step\\n    df.columns = col_names\\n    df = df.astype(dtype_out)\\n\\n    df[\"Type\"] = inputs[\"Type\"]\\n    return df',\n",
       " 'Import_Points': <function __main__.Import_Points(infile, inputs)>,\n",
       " '_i5': '#source data\\nrail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\\nrail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\\nferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nbus_infile = root_path + \"NaPTANcsv/Stops.csv\"\\nbus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\\n\\nacc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\\nacc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\\n\\nNSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\\nNSPL_inputs = {\"Type\":\"Postcodes\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\\n\\n#NSUL_Infile defined below\\nNSUL_inputs = {\"Type\":\"UDPRNs\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\\n\\npoint_pairs = [[rail_infile, rail_inputs],\\n[ferry_infile, ferry_inputs],\\n[bus_infile, bus_inputs],\\n[acc2020_infile, acc2020_inputs],\\n[NSPL_infile,NSPL_inputs]]\\n\\nNSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\\n\\npoint_pairs = point_pairs + NSUL_Pairs',\n",
       " 'rail_infile': 'D:/GeoData/NaPTANcsv/RailReferences.csv',\n",
       " 'rail_inputs': {'Type': 'RailwayStations',\n",
       "  'Name': 'StationName',\n",
       "  'Easting': 'Easting',\n",
       "  'Northing': 'Northing'},\n",
       " 'ferry_infile': 'D:/GeoData/NaPTANcsv/FerryReferences.csv',\n",
       " 'ferry_inputs': {'Type': 'FerryTerminals',\n",
       "  'Name': 'Name',\n",
       "  'Easting': 'Easting',\n",
       "  'Northing': 'Northing'},\n",
       " 'bus_infile': 'D:/GeoData/NaPTANcsv/Stops.csv',\n",
       " 'bus_inputs': {'Type': 'BusStops',\n",
       "  'Name': 'CommonName',\n",
       "  'Easting': 'Easting',\n",
       "  'Northing': 'Northing'},\n",
       " 'acc2020_infile': 'D:/GeoData/RoadSafety/dft-road-casualty-statistics-accident-2020.csv',\n",
       " 'acc2020_inputs': {'Type': 'RoadAccidents',\n",
       "  'Name': 'accident_reference',\n",
       "  'Easting': 'location_easting_osgr',\n",
       "  'Northing': 'location_northing_osgr'},\n",
       " 'NSPL_infile': 'D:/GeoData/NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv',\n",
       " 'NSPL_inputs': {'Type': 'Postcodes',\n",
       "  'Name': 'pcd',\n",
       "  'Easting': 'oseast1m',\n",
       "  'Northing': 'osnrth1m'},\n",
       " 'NSUL_inputs': {'Type': 'UDPRNs',\n",
       "  'Name': 'uprn',\n",
       "  'Easting': 'gridgb1e',\n",
       "  'Northing': 'gridgb1n',\n",
       "  'Details_Str': 'pcds'},\n",
       " 'point_pairs': [['D:/GeoData/NaPTANcsv/RailReferences.csv',\n",
       "   {'Type': 'RailwayStations',\n",
       "    'Name': 'StationName',\n",
       "    'Easting': 'Easting',\n",
       "    'Northing': 'Northing'}],\n",
       "  ['D:/GeoData/NaPTANcsv/FerryReferences.csv',\n",
       "   {'Type': 'FerryTerminals',\n",
       "    'Name': 'Name',\n",
       "    'Easting': 'Easting',\n",
       "    'Northing': 'Northing'}],\n",
       "  ['D:/GeoData/NaPTANcsv/Stops.csv',\n",
       "   {'Type': 'BusStops',\n",
       "    'Name': 'CommonName',\n",
       "    'Easting': 'Easting',\n",
       "    'Northing': 'Northing'}],\n",
       "  ['D:/GeoData/RoadSafety/dft-road-casualty-statistics-accident-2020.csv',\n",
       "   {'Type': 'RoadAccidents',\n",
       "    'Name': 'accident_reference',\n",
       "    'Easting': 'location_easting_osgr',\n",
       "    'Northing': 'location_northing_osgr'}],\n",
       "  ['D:/GeoData/NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv',\n",
       "   {'Type': 'Postcodes',\n",
       "    'Name': 'pcd',\n",
       "    'Easting': 'oseast1m',\n",
       "    'Northing': 'osnrth1m'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_EE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_EM.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_LN.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_NE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_NW.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SC.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SW.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_WA.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_WM.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_YH.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}]],\n",
       " 'NSUL_Pairs': [['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_EE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_EM.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_LN.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_NE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_NW.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SC.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SE.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_SW.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_WA.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_WM.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}],\n",
       "  ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_YH.csv',\n",
       "   {'Type': 'UDPRNs',\n",
       "    'Name': 'uprn',\n",
       "    'Easting': 'gridgb1e',\n",
       "    'Northing': 'gridgb1n',\n",
       "    'Details_Str': 'pcds'}]],\n",
       " '_i6': '#initiate the standard structure\\nPoints_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\\ndtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\\nPoints_of_Interest = Points_of_Interest.astype(dtypes)\\nPoints_of_Interest = Points_of_Interest.drop(0)',\n",
       " 'dtypes': {'Type': 'str',\n",
       "  'Name': 'str',\n",
       "  'Details_Str': 'str',\n",
       "  'Details_Float': numpy.float64,\n",
       "  'Easting': numpy.float64,\n",
       "  'Northing': numpy.float64},\n",
       " '_i7': 'for l in point_pairs:\\n    Points_of_Interest = Points_of_Interest.append(Import_Points(l[0], l[1]), ignore_index=True)\\n\\n#remove the spaces from the postcode\\nPoints_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", \"Name\"].astype(str).str.replace(\" \",\"\")',\n",
       " 'l': ['D:/GeoData/NSUL_OCT_2020/Data/NSUL_OCT_2020_YH.csv',\n",
       "  {'Type': 'UDPRNs',\n",
       "   'Name': 'uprn',\n",
       "   'Easting': 'gridgb1e',\n",
       "   'Northing': 'gridgb1n',\n",
       "   'Details_Str': 'pcds'}],\n",
       " '_VSCODE_json': <module 'json' from 'C:\\\\Users\\\\richa\\\\Anaconda3\\\\envs\\\\GeoData\\\\lib\\\\json\\\\__init__.py'>,\n",
       " '_VSCODE_builtins': <module 'builtins' (built-in)>,\n",
       " '_VSCODE_getVariableInfo': <function __main__._VSCODE_getVariableInfo(var)>,\n",
       " '_VSCODE_getVariableProperties': <function __main__._VSCODE_getVariableProperties(var, listOfAttributes)>,\n",
       " '_VSCODE_getVariableTypes': <function __main__._VSCODE_getVariableTypes(varnames)>,\n",
       " '_i8': '#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\\nLReg_Names = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\n              \\'Old_New\\', \\'Duration\\', \\'PAON\\', \\'SAON\\', \\'Street\\', \\'Locality\\', \\'Town_City\\', \\'District\\', \\'County\\', \\n              \\'PPDCategory_Type\\', \\'Record_Status_monthly_file_only\\']\\n\\nusecols = [\\'Transaction_unique_identifier\\', \\'Price\\', \\'Date_of_Transfer\\', \\'Postcode\\', \\'Property_Type\\', \\'Old_New\\', \\'Duration\\']\\n\\nLReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \\nLReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\\n\\nLReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\\n\\n#remove the spaces from the postcode\\nLReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nLReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nLReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\\n    LReg_Data[\"Property_Type\"].to_numpy(), \\n    LReg_Data[\"Old_New\"].to_numpy(), \\n    LReg_Data[\"Duration\"].to_numpy())\\n    ).tolist()\\n\\nLReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\\nLReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\\nLReg_Data[\"Type\"] = \"LReg\"\\nLReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\\ndel LReg_Data, LReg_Data2020, LReg_Data2021, LReg_Names, usecols',\n",
       " '_rwho_ls': ['Import_Points',\n",
       "  'Main_CRS',\n",
       "  'NSPL_infile',\n",
       "  'NSPL_inputs',\n",
       "  'NSUL_Pairs',\n",
       "  'NSUL_inputs',\n",
       "  'acc2020_infile',\n",
       "  'acc2020_inputs',\n",
       "  'bus_infile',\n",
       "  'bus_inputs',\n",
       "  'dirs',\n",
       "  'dtypes',\n",
       "  'ferry_infile',\n",
       "  'ferry_inputs',\n",
       "  'file',\n",
       "  'files',\n",
       "  'get_NE_Data',\n",
       "  'get_shapefile',\n",
       "  'getextra',\n",
       "  'glob',\n",
       "  'gpd',\n",
       "  'import_met',\n",
       "  'k',\n",
       "  'l',\n",
       "  'load_obj',\n",
       "  'name',\n",
       "  'np',\n",
       "  'os',\n",
       "  'pd',\n",
       "  'pickle',\n",
       "  'point_pairs',\n",
       "  'rail_infile',\n",
       "  'rail_inputs',\n",
       "  'raw_gdf',\n",
       "  'root',\n",
       "  'root_path',\n",
       "  'sys',\n",
       "  'v',\n",
       "  'x_points',\n",
       "  'xr',\n",
       "  'y_points'],\n",
       " '_i9': '#Firestation Data\\nFireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \\n                         engine=\"odf\",\\n                        sheet_name = \"STATIONS\")\\n\\nFireStations[\"Type\"] = \"FireStations\"\\nFireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\\nFireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\\nFireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\\nFireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\\ndel FireStations',\n",
       " '_i10': '#Schools Data\\nusecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\\nSchools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\\n\\n#remove the spaces from the postcode\\nSchools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\\n\\nSchools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"Postcodes\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\\n\\nSchools[\"Type\"] = \"Schools\"\\nSchools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\\nSchools[\"Name\"] = Schools[\"EstablishmentName\"]\\nSchools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\\n\\nPoints_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\\ndel Schools, usecols',\n",
       " '_i11': '#Computer doesn\\'t have enough ram to deal with UDPRN data so need to drop it here\\nPoints_of_Interest = Points_of_Interest[Points_of_Interest[\"Type\"]!=\"UDPRNs\"]',\n",
       " '_i12': '#We know that some of the easings and northings are on the Irish grid, split these out\\nNI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"Postcodes\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"UDPRNs\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\\nNI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\\n\\nNI = Points_of_Interest.loc[NI_Mask,:]\\nPoints_of_Interest = Points_of_Interest.loc[~NI_Mask,:]\\n\\ndel NI_Mask_1, NI_Mask_2, NI_Mask',\n",
       " '_i13': '#Convert to Geopandas dataframe\\nx_points = Points_of_Interest[\"Easting\"].to_numpy()\\ny_points = Points_of_Interest[\"Northing\"].to_numpy()\\n\\nraw_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\ndel Points_of_Interest, x_points, y_points\\n\\n#Do the same for NI, and convert to the appropriate crs\\nx_points = NI[\"Easting\"].to_numpy()\\ny_points = NI[\"Northing\"].to_numpy()\\n\\nNI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\\nNI_gdf = NI_gdf.to_crs(Main_CRS)\\ndel NI, x_points, y_points\\n\\n#Re-combine now they are on the same coordinates\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, NI_gdf], ignore_index=True) )\\ndel NI_gdf',\n",
       " 'raw_gdf':                      Type                             Name Details_Str  \\\n",
       " 0         RailwayStations            Penzance Rail Station         NaN   \n",
       " 1         RailwayStations  St Ives (Cornwall) Rail Station         NaN   \n",
       " 2         RailwayStations          Carbis Bay Rail Station         NaN   \n",
       " 3         RailwayStations             St Erth Rail Station         NaN   \n",
       " 4         RailwayStations     Lelant Saltings Rail Station         NaN   \n",
       " ...                   ...                              ...         ...   \n",
       " 11937399        MetOffice            GroundFrost_Days_2020         NaN   \n",
       " 11937400        MetOffice            GroundFrost_Days_2020         NaN   \n",
       " 11937401        MetOffice            GroundFrost_Days_2020         NaN   \n",
       " 11937402        MetOffice            GroundFrost_Days_2020         NaN   \n",
       " 11937403        MetOffice            GroundFrost_Days_2020         NaN   \n",
       " \n",
       "           Details_Float                        geometry fictitious identifier  \\\n",
       " 0                   NaN    POINT (147588.000 30599.000)        NaN        NaN   \n",
       " 1                   NaN    POINT (151947.000 40127.000)        NaN        NaN   \n",
       " 2                   NaN    POINT (152930.000 38745.000)        NaN        NaN   \n",
       " 3                   NaN    POINT (154150.000 35730.000)        NaN        NaN   \n",
       " 4                   NaN    POINT (154430.000 36640.000)        NaN        NaN   \n",
       " ...                 ...                             ...        ...        ...   \n",
       " 11937399      32.727988  POINT (464500.000 1216500.000)        NaN        NaN   \n",
       " 11937400      31.768013  POINT (465500.000 1216500.000)        NaN        NaN   \n",
       " 11937401      31.514997  POINT (460500.000 1217500.000)        NaN        NaN   \n",
       " 11937402      32.007471  POINT (461500.000 1217500.000)        NaN        NaN   \n",
       " 11937403      31.221822  POINT (463500.000 1217500.000)        NaN        NaN   \n",
       " \n",
       "          class roadNumber name1  ... primary trunkRoad loop startNode  \\\n",
       " 0          NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 1          NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 2          NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 3          NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 4          NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " ...        ...        ...   ...  ...     ...       ...  ...       ...   \n",
       " 11937399   NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 11937400   NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 11937401   NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 11937402   NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " 11937403   NaN        NaN   NaN  ...     NaN       NaN  NaN       NaN   \n",
       " \n",
       "           endNode structure nameTOID numberTOID function number  \n",
       " 0             NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 1             NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 2             NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 3             NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 4             NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " ...           ...       ...      ...        ...      ...    ...  \n",
       " 11937399      NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 11937400      NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 11937401      NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 11937402      NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " 11937403      NaN       NaN      NaN        NaN      NaN    NaN  \n",
       " \n",
       " [11937404 rows x 25 columns],\n",
       " '_i14': '#Police Crime data\\nusecols = [\"Crime type\", \"Month\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64, \"Month\":\"str\"}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"street.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nCrimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\\n\\nfor file in infile[1:]:\\n    Crimes=Crimes.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\\n\\nCrimes = Crimes.dropna()\\nCrimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\\nCrimes = Crimes.rename(columns={\"Month\": \"Details_Str\"})\\nCrimes[\"Type\"] = \"Crimes\"\\n\\n#Police Stop and Search Data\\nusecols = [\"Object of search\", \"Date\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nfile_list = []\\nfor root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\\n    for name in files:\\n        if name.endswith((\"search.csv\")):\\n            file_list = file_list + [str(root) + \"/\" + str(name)]\\n\\ninfile = file_list[0]\\n\\nStopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"])\\n\\nfor file in file_list[1:]:\\n    StopAndSearch=StopAndSearch.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols, parse_dates=[\"Date\"]), ignore_index=True)\\n\\nStopAndSearch = StopAndSearch.dropna()\\nStopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\\nStopAndSearch[\"Date\"] = StopAndSearch[\"Date\"].dt.strftime(\\'%Y-%m-%d\\')\\nStopAndSearch = StopAndSearch.rename(columns={\"Date\": \"Details_Str\"})\\nStopAndSearch[\"Type\"] = \"StopAndSearch\"\\n\\nCrimes = Crimes.append(StopAndSearch,ignore_index=True)\\ndel StopAndSearch\\n\\n#Convert to Geopandas dataframe\\nx_points = Crimes[\"Longitude\"].to_numpy()\\ny_points = Crimes[\"Latitude\"].to_numpy()\\n\\ncrimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\ncrimes_gdf = crimes_gdf.to_crs(Main_CRS)\\ncrimes_gdf\\n\\ndel Crimes\\n\\n#combine with the main points gdf\\nraw_gdf = gpd.GeoDataFrame( pd.concat( [raw_gdf, crimes_gdf], ignore_index=True) )\\ndel crimes_gdf, file_list, infile, usecols, in_dtypes',\n",
       " 'root': 'D:/GeoData/PoliceData/2021-11',\n",
       " 'dirs': [],\n",
       " 'files': ['2021-11-avon-and-somerset-outcomes.csv',\n",
       "  '2021-11-avon-and-somerset-stop-and-search.csv',\n",
       "  '2021-11-avon-and-somerset-street.csv',\n",
       "  '2021-11-bedfordshire-outcomes.csv',\n",
       "  '2021-11-bedfordshire-stop-and-search.csv',\n",
       "  '2021-11-bedfordshire-street.csv',\n",
       "  '2021-11-btp-stop-and-search.csv',\n",
       "  '2021-11-btp-street.csv',\n",
       "  '2021-11-cambridgeshire-outcomes.csv',\n",
       "  '2021-11-cambridgeshire-stop-and-search.csv',\n",
       "  '2021-11-cambridgeshire-street.csv',\n",
       "  '2021-11-cheshire-outcomes.csv',\n",
       "  '2021-11-cheshire-street.csv',\n",
       "  '2021-11-city-of-london-outcomes.csv',\n",
       "  '2021-11-city-of-london-stop-and-search.csv',\n",
       "  '2021-11-city-of-london-street.csv',\n",
       "  '2021-11-cleveland-outcomes.csv',\n",
       "  '2021-11-cleveland-stop-and-search.csv',\n",
       "  '2021-11-cleveland-street.csv',\n",
       "  '2021-11-cumbria-outcomes.csv',\n",
       "  '2021-11-cumbria-stop-and-search.csv',\n",
       "  '2021-11-cumbria-street.csv',\n",
       "  '2021-11-derbyshire-outcomes.csv',\n",
       "  '2021-11-derbyshire-stop-and-search.csv',\n",
       "  '2021-11-derbyshire-street.csv',\n",
       "  '2021-11-devon-and-cornwall-outcomes.csv',\n",
       "  '2021-11-devon-and-cornwall-street.csv',\n",
       "  '2021-11-dorset-outcomes.csv',\n",
       "  '2021-11-dorset-stop-and-search.csv',\n",
       "  '2021-11-dorset-street.csv',\n",
       "  '2021-11-durham-outcomes.csv',\n",
       "  '2021-11-durham-stop-and-search.csv',\n",
       "  '2021-11-durham-street.csv',\n",
       "  '2021-11-dyfed-powys-outcomes.csv',\n",
       "  '2021-11-dyfed-powys-stop-and-search.csv',\n",
       "  '2021-11-dyfed-powys-street.csv',\n",
       "  '2021-11-essex-outcomes.csv',\n",
       "  '2021-11-essex-stop-and-search.csv',\n",
       "  '2021-11-essex-street.csv',\n",
       "  '2021-11-gloucestershire-outcomes.csv',\n",
       "  '2021-11-gloucestershire-stop-and-search.csv',\n",
       "  '2021-11-gloucestershire-street.csv',\n",
       "  '2021-11-gwent-outcomes.csv',\n",
       "  '2021-11-gwent-stop-and-search.csv',\n",
       "  '2021-11-gwent-street.csv',\n",
       "  '2021-11-hampshire-outcomes.csv',\n",
       "  '2021-11-hampshire-street.csv',\n",
       "  '2021-11-hertfordshire-outcomes.csv',\n",
       "  '2021-11-hertfordshire-stop-and-search.csv',\n",
       "  '2021-11-hertfordshire-street.csv',\n",
       "  '2021-11-humberside-outcomes.csv',\n",
       "  '2021-11-humberside-stop-and-search.csv',\n",
       "  '2021-11-humberside-street.csv',\n",
       "  '2021-11-kent-outcomes.csv',\n",
       "  '2021-11-kent-stop-and-search.csv',\n",
       "  '2021-11-kent-street.csv',\n",
       "  '2021-11-lancashire-outcomes.csv',\n",
       "  '2021-11-lancashire-stop-and-search.csv',\n",
       "  '2021-11-lancashire-street.csv',\n",
       "  '2021-11-leicestershire-outcomes.csv',\n",
       "  '2021-11-leicestershire-stop-and-search.csv',\n",
       "  '2021-11-leicestershire-street.csv',\n",
       "  '2021-11-lincolnshire-outcomes.csv',\n",
       "  '2021-11-lincolnshire-street.csv',\n",
       "  '2021-11-metropolitan-outcomes.csv',\n",
       "  '2021-11-metropolitan-street.csv',\n",
       "  '2021-11-norfolk-outcomes.csv',\n",
       "  '2021-11-norfolk-stop-and-search.csv',\n",
       "  '2021-11-norfolk-street.csv',\n",
       "  '2021-11-north-wales-outcomes.csv',\n",
       "  '2021-11-north-wales-stop-and-search.csv',\n",
       "  '2021-11-north-wales-street.csv',\n",
       "  '2021-11-north-yorkshire-outcomes.csv',\n",
       "  '2021-11-north-yorkshire-stop-and-search.csv',\n",
       "  '2021-11-north-yorkshire-street.csv',\n",
       "  '2021-11-northamptonshire-outcomes.csv',\n",
       "  '2021-11-northamptonshire-stop-and-search.csv',\n",
       "  '2021-11-northamptonshire-street.csv',\n",
       "  '2021-11-northern-ireland-street.csv',\n",
       "  '2021-11-northumbria-outcomes.csv',\n",
       "  '2021-11-northumbria-stop-and-search.csv',\n",
       "  '2021-11-northumbria-street.csv',\n",
       "  '2021-11-nottinghamshire-outcomes.csv',\n",
       "  '2021-11-nottinghamshire-stop-and-search.csv',\n",
       "  '2021-11-nottinghamshire-street.csv',\n",
       "  '2021-11-south-wales-outcomes.csv',\n",
       "  '2021-11-south-wales-stop-and-search.csv',\n",
       "  '2021-11-south-wales-street.csv',\n",
       "  '2021-11-south-yorkshire-outcomes.csv',\n",
       "  '2021-11-south-yorkshire-stop-and-search.csv',\n",
       "  '2021-11-south-yorkshire-street.csv',\n",
       "  '2021-11-staffordshire-outcomes.csv',\n",
       "  '2021-11-staffordshire-stop-and-search.csv',\n",
       "  '2021-11-staffordshire-street.csv',\n",
       "  '2021-11-suffolk-outcomes.csv',\n",
       "  '2021-11-suffolk-stop-and-search.csv',\n",
       "  '2021-11-suffolk-street.csv',\n",
       "  '2021-11-surrey-outcomes.csv',\n",
       "  '2021-11-surrey-stop-and-search.csv',\n",
       "  '2021-11-surrey-street.csv',\n",
       "  '2021-11-sussex-outcomes.csv',\n",
       "  '2021-11-sussex-stop-and-search.csv',\n",
       "  '2021-11-sussex-street.csv',\n",
       "  '2021-11-thames-valley-outcomes.csv',\n",
       "  '2021-11-thames-valley-stop-and-search.csv',\n",
       "  '2021-11-thames-valley-street.csv',\n",
       "  '2021-11-warwickshire-outcomes.csv',\n",
       "  '2021-11-warwickshire-stop-and-search.csv',\n",
       "  '2021-11-warwickshire-street.csv',\n",
       "  '2021-11-west-mercia-outcomes.csv',\n",
       "  '2021-11-west-mercia-stop-and-search.csv',\n",
       "  '2021-11-west-mercia-street.csv',\n",
       "  '2021-11-west-yorkshire-outcomes.csv',\n",
       "  '2021-11-west-yorkshire-stop-and-search.csv',\n",
       "  '2021-11-west-yorkshire-street.csv',\n",
       "  '2021-11-wiltshire-outcomes.csv',\n",
       "  '2021-11-wiltshire-street.csv'],\n",
       " 'name': '2021-11-wiltshire-street.csv',\n",
       " 'x_points': array([-2.88441372, -2.80087352, -2.89056897, ..., -0.50070226,\n",
       "        -1.54612899, -0.41093534]),\n",
       " 'y_points': array([53.48259354, 53.41563034, 53.48220444, ..., 53.24242783,\n",
       "        52.91253281, 51.86693192]),\n",
       " '_i15': '#NHS Data\\n\\nusecols = [\"OrganisationType\", \"OrganisationName\", \"Longitude\", \"Latitude\"]\\nin_dtypes = {\"OrganisationType\":\"str\", \"OrganisationName\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\\n\\nNHS_list = [root_path + \"NHS/\" + str(i) for i in os.listdir(\"D:/GeoData/NHS\") if \".csv\" in i]\\n\\nNHS = pd.read_csv(NHS_list[0], sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes)\\n\\nfor file in NHS_list[1:]:\\n    NHS = NHS.append(pd.read_csv(file, sep=\\'¬\\'  , engine=\\'python\\', encoding = \"ISO-8859-1\", usecols=usecols, dtype=in_dtypes), ignore_index=True)\\n\\nNHS = NHS.rename(columns={\"OrganisationType\":\"Type\", \"OrganisationName\":\"Name\"})\\n\\n#Convert to Geopandas dataframe\\nx_points = NHS[\"Longitude\"].to_numpy()\\ny_points = NHS[\"Latitude\"].to_numpy()\\n\\nNHS_gdf = gpd.GeoDataFrame(NHS.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\\nNHS_gdf = NHS_gdf.to_crs(Main_CRS)\\nNHS_gdf\\n\\ndel NHS\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, NHS_gdf], ignore_index=True) )\\n\\ndel NHS_gdf, usecols, in_dtypes, NHS_list',\n",
       " '_i16': 'Shape_Loc = {\\n    \\'All_GB\\' : [root_path + \"Countries__December_2019__Boundaries_UK_BFC-shp/Countries__December_2019__Boundaries_UK_BFC.shp\", \"ctry19nm\"],\\n    \\'National_Parks\\' : [root_path + \"National_Parks__December_2019__GB_BFE-shp/National_Parks__December_2019__GB_BFE.shp\", \"NPARK19NM\"],\\n    \\'LocalAuthorities\\' : [root_path + \"Local_Authority_Districts__May_2020__Boundaries_UK_BFC-shp/Local_Authority_Districts__May_2020__Boundaries_UK_BFC.shp\", \"LAD20NM\"],\\n    \\'LSOA\\' : [root_path + \"Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2-shp/Lower_Layer_Super_Output_Area__December_2011__EW_BSC_V2.shp\", \"LSOA11NMW\"],\\n    \\'GreenSpace\\' : [root_path + \"opgrsp_essh_gb/OS Open Greenspace (ESRI Shape File) GB/data/GB_GreenspaceSite.shp\", \"id\"],\\n    \\'Rivers\\' : [root_path + \"oprvrs_essh_gb/data/WatercourseLink.shp\",\"name1\"],\\n    \\'Railway_Lines\\' : [root_path + \"strtgi_essh_gb/data/railway_line.shp\",\"LEGEND\"],\\n    \\'Woodland_Region\\' : [root_path + \"strtgi_essh_gb/data/woodland_region.shp\",\"LEGEND\"],\\n    \\'Urban_Region\\' : [root_path + \"strtgi_essh_gb/data/urban_region.shp\",\"LEGEND\"],\\n    \\'Foreshor_Region\\' : [root_path + \"strtgi_essh_gb/data/foreshor_region.shp\",\"LEGEND\"],\\n    \\'Ferry_Line\\' : [root_path + \"strtgi_essh_gb/data/ferry_line.shp\",\"LEGEND\", [\"FERRY_TIME\", \"FERRY_TYPE\", \"RESTRICTIO\", \"ACCESS\"]],\\n    \\'Coastline\\' : [root_path + \"strtgi_essh_gb/data/coastline.shp\",\"LEGEND\"],\\n    \\'Lakes\\' : [root_path + \"strtgi_essh_gb/data/lakes_region.shp\", \"LEGEND\"]}',\n",
       " '_i17': 'def getextra(list):\\n    try:\\n        extra = list[2]\\n        extraexists = True\\n    except:\\n        extra = []\\n        extraexists = False\\n\\n    return extra, extraexists\\n\\ndef get_shapefile(v):\\n    extra, extraexists = getextra(v)\\n    gdf = gpd.read_file(v[0]).to_crs(Main_CRS).loc[:,[v[1], \"geometry\"]+extra]\\n    gdf = gdf.rename(columns={v[1]: \"Name\"})\\n    gdf[\"Type\"] = k\\n\\n    usecols = [\"Type\", \"Name\", \"geometry\"]\\n\\n    if extraexists:\\n        array = gdf[extra[0]].to_numpy()\\n        if len(extra)>1:\\n            for e in extra[1:]:\\n                array = np.column_stack((array,gdf[e].to_numpy()))\\n        gdf[\"Details_Str\"] = array.tolist()\\n        usecols = usecols + [\"Details_Str\"]\\n\\n    gdf = gdf[usecols]\\n\\n    return gdf',\n",
       " 'getextra': <function __main__.getextra(list)>,\n",
       " 'get_shapefile': <function __main__.get_shapefile(v)>,\n",
       " '_i18': '#Add all of the shape files to the master list\\n\\nfor k in Shape_Loc.keys():\\n    v = Shape_Loc[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_shapefile(v)], ignore_index=True) )\\n\\ndel Shape_Loc',\n",
       " 'k': 'groundfrost',\n",
       " 'v': {'url': 'https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson',\n",
       "  'Type': 'AncientWoodland',\n",
       "  'Name': 'NAME',\n",
       "  'Details_Str': 'THEMNAME',\n",
       "  'geometry': 'geometry'},\n",
       " '_i19': '#add the roads\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\n\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"RoadLink.shp\" in i]\\n\\nRoads = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path], ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, Roads], ignore_index=True) )\\ndel Roads, file, path',\n",
       " '_i20': '#Add the motorway junctions\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       " '_i21': '#Add the motorway junctions\\nfile = os.listdir(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\")\\npath = [os.path.join(\"D:\\\\GeoData\\\\oproad_essh_gb\\\\data\", i) for i in file if \"MotorwayJunction.shp\" in i]\\n\\nMotorwayJunctions = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path],  ignore_index=True), crs=Main_CRS)\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, MotorwayJunctions], ignore_index=True) )\\ndel MotorwayJunctions, path',\n",
       " 'file': ['HP_RoadLink.dbf',\n",
       "  'HP_RoadLink.prj',\n",
       "  'HP_RoadLink.shp',\n",
       "  'HP_RoadLink.shx',\n",
       "  'HP_RoadNode.dbf',\n",
       "  'HP_RoadNode.prj',\n",
       "  'HP_RoadNode.shp',\n",
       "  'HP_RoadNode.shx',\n",
       "  'HT_RoadLink.dbf',\n",
       "  'HT_RoadLink.prj',\n",
       "  'HT_RoadLink.shp',\n",
       "  'HT_RoadLink.shx',\n",
       "  'HT_RoadNode.dbf',\n",
       "  'HT_RoadNode.prj',\n",
       "  'HT_RoadNode.shp',\n",
       "  'HT_RoadNode.shx',\n",
       "  'HU_RoadLink.dbf',\n",
       "  'HU_RoadLink.prj',\n",
       "  'HU_RoadLink.shp',\n",
       "  'HU_RoadLink.shx',\n",
       "  'HU_RoadNode.dbf',\n",
       "  'HU_RoadNode.prj',\n",
       "  'HU_RoadNode.shp',\n",
       "  'HU_RoadNode.shx',\n",
       "  'HY_RoadLink.dbf',\n",
       "  'HY_RoadLink.prj',\n",
       "  'HY_RoadLink.shp',\n",
       "  'HY_RoadLink.shx',\n",
       "  'HY_RoadNode.dbf',\n",
       "  'HY_RoadNode.prj',\n",
       "  'HY_RoadNode.shp',\n",
       "  'HY_RoadNode.shx',\n",
       "  'HZ_RoadLink.dbf',\n",
       "  'HZ_RoadLink.prj',\n",
       "  'HZ_RoadLink.shp',\n",
       "  'HZ_RoadLink.shx',\n",
       "  'HZ_RoadNode.dbf',\n",
       "  'HZ_RoadNode.prj',\n",
       "  'HZ_RoadNode.shp',\n",
       "  'HZ_RoadNode.shx',\n",
       "  'NA_RoadLink.dbf',\n",
       "  'NA_RoadLink.prj',\n",
       "  'NA_RoadLink.shp',\n",
       "  'NA_RoadLink.shx',\n",
       "  'NA_RoadNode.dbf',\n",
       "  'NA_RoadNode.prj',\n",
       "  'NA_RoadNode.shp',\n",
       "  'NA_RoadNode.shx',\n",
       "  'NB_RoadLink.dbf',\n",
       "  'NB_RoadLink.prj',\n",
       "  'NB_RoadLink.shp',\n",
       "  'NB_RoadLink.shx',\n",
       "  'NB_RoadNode.dbf',\n",
       "  'NB_RoadNode.prj',\n",
       "  'NB_RoadNode.shp',\n",
       "  'NB_RoadNode.shx',\n",
       "  'NC_RoadLink.dbf',\n",
       "  'NC_RoadLink.prj',\n",
       "  'NC_RoadLink.shp',\n",
       "  'NC_RoadLink.shx',\n",
       "  'NC_RoadNode.dbf',\n",
       "  'NC_RoadNode.prj',\n",
       "  'NC_RoadNode.shp',\n",
       "  'NC_RoadNode.shx',\n",
       "  'ND_RoadLink.dbf',\n",
       "  'ND_RoadLink.prj',\n",
       "  'ND_RoadLink.shp',\n",
       "  'ND_RoadLink.shx',\n",
       "  'ND_RoadNode.dbf',\n",
       "  'ND_RoadNode.prj',\n",
       "  'ND_RoadNode.shp',\n",
       "  'ND_RoadNode.shx',\n",
       "  'NF_RoadLink.dbf',\n",
       "  'NF_RoadLink.prj',\n",
       "  'NF_RoadLink.shp',\n",
       "  'NF_RoadLink.shx',\n",
       "  'NF_RoadNode.dbf',\n",
       "  'NF_RoadNode.prj',\n",
       "  'NF_RoadNode.shp',\n",
       "  'NF_RoadNode.shx',\n",
       "  'NG_RoadLink.dbf',\n",
       "  'NG_RoadLink.prj',\n",
       "  'NG_RoadLink.shp',\n",
       "  'NG_RoadLink.shx',\n",
       "  'NG_RoadNode.dbf',\n",
       "  'NG_RoadNode.prj',\n",
       "  'NG_RoadNode.shp',\n",
       "  'NG_RoadNode.shx',\n",
       "  'NH_RoadLink.dbf',\n",
       "  'NH_RoadLink.prj',\n",
       "  'NH_RoadLink.shp',\n",
       "  'NH_RoadLink.shx',\n",
       "  'NH_RoadNode.dbf',\n",
       "  'NH_RoadNode.prj',\n",
       "  'NH_RoadNode.shp',\n",
       "  'NH_RoadNode.shx',\n",
       "  'NJ_RoadLink.dbf',\n",
       "  'NJ_RoadLink.prj',\n",
       "  'NJ_RoadLink.shp',\n",
       "  'NJ_RoadLink.shx',\n",
       "  'NJ_RoadNode.dbf',\n",
       "  'NJ_RoadNode.prj',\n",
       "  'NJ_RoadNode.shp',\n",
       "  'NJ_RoadNode.shx',\n",
       "  'NK_RoadLink.dbf',\n",
       "  'NK_RoadLink.prj',\n",
       "  'NK_RoadLink.shp',\n",
       "  'NK_RoadLink.shx',\n",
       "  'NK_RoadNode.dbf',\n",
       "  'NK_RoadNode.prj',\n",
       "  'NK_RoadNode.shp',\n",
       "  'NK_RoadNode.shx',\n",
       "  'NL_RoadLink.dbf',\n",
       "  'NL_RoadLink.prj',\n",
       "  'NL_RoadLink.shp',\n",
       "  'NL_RoadLink.shx',\n",
       "  'NL_RoadNode.dbf',\n",
       "  'NL_RoadNode.prj',\n",
       "  'NL_RoadNode.shp',\n",
       "  'NL_RoadNode.shx',\n",
       "  'NM_RoadLink.dbf',\n",
       "  'NM_RoadLink.prj',\n",
       "  'NM_RoadLink.shp',\n",
       "  'NM_RoadLink.shx',\n",
       "  'NM_RoadNode.dbf',\n",
       "  'NM_RoadNode.prj',\n",
       "  'NM_RoadNode.shp',\n",
       "  'NM_RoadNode.shx',\n",
       "  'NN_RoadLink.dbf',\n",
       "  'NN_RoadLink.prj',\n",
       "  'NN_RoadLink.shp',\n",
       "  'NN_RoadLink.shx',\n",
       "  'NN_RoadNode.dbf',\n",
       "  'NN_RoadNode.prj',\n",
       "  'NN_RoadNode.shp',\n",
       "  'NN_RoadNode.shx',\n",
       "  'NO_MotorwayJunction.dbf',\n",
       "  'NO_MotorwayJunction.prj',\n",
       "  'NO_MotorwayJunction.shp',\n",
       "  'NO_MotorwayJunction.shx',\n",
       "  'NO_RoadLink.dbf',\n",
       "  'NO_RoadLink.prj',\n",
       "  'NO_RoadLink.shp',\n",
       "  'NO_RoadLink.shx',\n",
       "  'NO_RoadNode.dbf',\n",
       "  'NO_RoadNode.prj',\n",
       "  'NO_RoadNode.shp',\n",
       "  'NO_RoadNode.shx',\n",
       "  'NR_RoadLink.dbf',\n",
       "  'NR_RoadLink.prj',\n",
       "  'NR_RoadLink.shp',\n",
       "  'NR_RoadLink.shx',\n",
       "  'NR_RoadNode.dbf',\n",
       "  'NR_RoadNode.prj',\n",
       "  'NR_RoadNode.shp',\n",
       "  'NR_RoadNode.shx',\n",
       "  'NS_MotorwayJunction.dbf',\n",
       "  'NS_MotorwayJunction.prj',\n",
       "  'NS_MotorwayJunction.shp',\n",
       "  'NS_MotorwayJunction.shx',\n",
       "  'NS_RoadLink.dbf',\n",
       "  'NS_RoadLink.prj',\n",
       "  'NS_RoadLink.shp',\n",
       "  'NS_RoadLink.shx',\n",
       "  'NS_RoadNode.dbf',\n",
       "  'NS_RoadNode.prj',\n",
       "  'NS_RoadNode.shp',\n",
       "  'NS_RoadNode.shx',\n",
       "  'NT_MotorwayJunction.dbf',\n",
       "  'NT_MotorwayJunction.prj',\n",
       "  'NT_MotorwayJunction.shp',\n",
       "  'NT_MotorwayJunction.shx',\n",
       "  'NT_RoadLink.dbf',\n",
       "  'NT_RoadLink.prj',\n",
       "  'NT_RoadLink.shp',\n",
       "  'NT_RoadLink.shx',\n",
       "  'NT_RoadNode.dbf',\n",
       "  'NT_RoadNode.prj',\n",
       "  'NT_RoadNode.shp',\n",
       "  'NT_RoadNode.shx',\n",
       "  'NU_RoadLink.dbf',\n",
       "  'NU_RoadLink.prj',\n",
       "  'NU_RoadLink.shp',\n",
       "  'NU_RoadLink.shx',\n",
       "  'NU_RoadNode.dbf',\n",
       "  'NU_RoadNode.prj',\n",
       "  'NU_RoadNode.shp',\n",
       "  'NU_RoadNode.shx',\n",
       "  'NW_RoadLink.dbf',\n",
       "  'NW_RoadLink.prj',\n",
       "  'NW_RoadLink.shp',\n",
       "  'NW_RoadLink.shx',\n",
       "  'NW_RoadNode.dbf',\n",
       "  'NW_RoadNode.prj',\n",
       "  'NW_RoadNode.shp',\n",
       "  'NW_RoadNode.shx',\n",
       "  'NX_RoadLink.dbf',\n",
       "  'NX_RoadLink.prj',\n",
       "  'NX_RoadLink.shp',\n",
       "  'NX_RoadLink.shx',\n",
       "  'NX_RoadNode.dbf',\n",
       "  'NX_RoadNode.prj',\n",
       "  'NX_RoadNode.shp',\n",
       "  'NX_RoadNode.shx',\n",
       "  'NY_MotorwayJunction.dbf',\n",
       "  'NY_MotorwayJunction.prj',\n",
       "  'NY_MotorwayJunction.shp',\n",
       "  'NY_MotorwayJunction.shx',\n",
       "  'NY_RoadLink.dbf',\n",
       "  'NY_RoadLink.prj',\n",
       "  'NY_RoadLink.shp',\n",
       "  'NY_RoadLink.shx',\n",
       "  'NY_RoadNode.dbf',\n",
       "  'NY_RoadNode.prj',\n",
       "  'NY_RoadNode.shp',\n",
       "  'NY_RoadNode.shx',\n",
       "  'NZ_MotorwayJunction.dbf',\n",
       "  'NZ_MotorwayJunction.prj',\n",
       "  'NZ_MotorwayJunction.shp',\n",
       "  'NZ_MotorwayJunction.shx',\n",
       "  'NZ_RoadLink.dbf',\n",
       "  'NZ_RoadLink.prj',\n",
       "  'NZ_RoadLink.shp',\n",
       "  'NZ_RoadLink.shx',\n",
       "  'NZ_RoadNode.dbf',\n",
       "  'NZ_RoadNode.prj',\n",
       "  'NZ_RoadNode.shp',\n",
       "  'NZ_RoadNode.shx',\n",
       "  'SD_MotorwayJunction.dbf',\n",
       "  'SD_MotorwayJunction.prj',\n",
       "  'SD_MotorwayJunction.shp',\n",
       "  'SD_MotorwayJunction.shx',\n",
       "  'SD_RoadLink.dbf',\n",
       "  'SD_RoadLink.prj',\n",
       "  'SD_RoadLink.shp',\n",
       "  'SD_RoadLink.shx',\n",
       "  'SD_RoadNode.dbf',\n",
       "  'SD_RoadNode.prj',\n",
       "  'SD_RoadNode.shp',\n",
       "  'SD_RoadNode.shx',\n",
       "  'SE_MotorwayJunction.dbf',\n",
       "  'SE_MotorwayJunction.prj',\n",
       "  'SE_MotorwayJunction.shp',\n",
       "  'SE_MotorwayJunction.shx',\n",
       "  'SE_RoadLink.dbf',\n",
       "  'SE_RoadLink.prj',\n",
       "  'SE_RoadLink.shp',\n",
       "  'SE_RoadLink.shx',\n",
       "  'SE_RoadNode.dbf',\n",
       "  'SE_RoadNode.prj',\n",
       "  'SE_RoadNode.shp',\n",
       "  'SE_RoadNode.shx',\n",
       "  'SH_RoadLink.dbf',\n",
       "  'SH_RoadLink.prj',\n",
       "  'SH_RoadLink.shp',\n",
       "  'SH_RoadLink.shx',\n",
       "  'SH_RoadNode.dbf',\n",
       "  'SH_RoadNode.prj',\n",
       "  'SH_RoadNode.shp',\n",
       "  'SH_RoadNode.shx',\n",
       "  'SJ_MotorwayJunction.dbf',\n",
       "  'SJ_MotorwayJunction.prj',\n",
       "  'SJ_MotorwayJunction.shp',\n",
       "  'SJ_MotorwayJunction.shx',\n",
       "  'SJ_RoadLink.dbf',\n",
       "  'SJ_RoadLink.prj',\n",
       "  'SJ_RoadLink.shp',\n",
       "  'SJ_RoadLink.shx',\n",
       "  'SJ_RoadNode.dbf',\n",
       "  'SJ_RoadNode.prj',\n",
       "  'SJ_RoadNode.shp',\n",
       "  'SJ_RoadNode.shx',\n",
       "  'SK_MotorwayJunction.dbf',\n",
       "  'SK_MotorwayJunction.prj',\n",
       "  'SK_MotorwayJunction.shp',\n",
       "  'SK_MotorwayJunction.shx',\n",
       "  'SK_RoadLink.dbf',\n",
       "  'SK_RoadLink.prj',\n",
       "  'SK_RoadLink.shp',\n",
       "  'SK_RoadLink.shx',\n",
       "  'SK_RoadNode.dbf',\n",
       "  'SK_RoadNode.prj',\n",
       "  'SK_RoadNode.shp',\n",
       "  'SK_RoadNode.shx',\n",
       "  'SM_RoadLink.dbf',\n",
       "  'SM_RoadLink.prj',\n",
       "  'SM_RoadLink.shp',\n",
       "  'SM_RoadLink.shx',\n",
       "  'SM_RoadNode.dbf',\n",
       "  'SM_RoadNode.prj',\n",
       "  'SM_RoadNode.shp',\n",
       "  'SM_RoadNode.shx',\n",
       "  'SN_MotorwayJunction.dbf',\n",
       "  'SN_MotorwayJunction.prj',\n",
       "  'SN_MotorwayJunction.shp',\n",
       "  'SN_MotorwayJunction.shx',\n",
       "  'SN_RoadLink.dbf',\n",
       "  'SN_RoadLink.prj',\n",
       "  'SN_RoadLink.shp',\n",
       "  'SN_RoadLink.shx',\n",
       "  'SN_RoadNode.dbf',\n",
       "  'SN_RoadNode.prj',\n",
       "  'SN_RoadNode.shp',\n",
       "  'SN_RoadNode.shx',\n",
       "  'SO_MotorwayJunction.dbf',\n",
       "  'SO_MotorwayJunction.prj',\n",
       "  'SO_MotorwayJunction.shp',\n",
       "  'SO_MotorwayJunction.shx',\n",
       "  'SO_RoadLink.dbf',\n",
       "  'SO_RoadLink.prj',\n",
       "  'SO_RoadLink.shp',\n",
       "  'SO_RoadLink.shx',\n",
       "  'SO_RoadNode.dbf',\n",
       "  'SO_RoadNode.prj',\n",
       "  'SO_RoadNode.shp',\n",
       "  'SO_RoadNode.shx',\n",
       "  'SP_MotorwayJunction.dbf',\n",
       "  'SP_MotorwayJunction.prj',\n",
       "  'SP_MotorwayJunction.shp',\n",
       "  'SP_MotorwayJunction.shx',\n",
       "  'SP_RoadLink.dbf',\n",
       "  'SP_RoadLink.prj',\n",
       "  'SP_RoadLink.shp',\n",
       "  'SP_RoadLink.shx',\n",
       "  'SP_RoadNode.dbf',\n",
       "  'SP_RoadNode.prj',\n",
       "  'SP_RoadNode.shp',\n",
       "  'SP_RoadNode.shx',\n",
       "  'SR_RoadLink.dbf',\n",
       "  'SR_RoadLink.prj',\n",
       "  'SR_RoadLink.shp',\n",
       "  'SR_RoadLink.shx',\n",
       "  'SR_RoadNode.dbf',\n",
       "  'SR_RoadNode.prj',\n",
       "  'SR_RoadNode.shp',\n",
       "  'SR_RoadNode.shx',\n",
       "  'SS_MotorwayJunction.dbf',\n",
       "  'SS_MotorwayJunction.prj',\n",
       "  'SS_MotorwayJunction.shp',\n",
       "  'SS_MotorwayJunction.shx',\n",
       "  'SS_RoadLink.dbf',\n",
       "  'SS_RoadLink.prj',\n",
       "  'SS_RoadLink.shp',\n",
       "  'SS_RoadLink.shx',\n",
       "  'SS_RoadNode.dbf',\n",
       "  'SS_RoadNode.prj',\n",
       "  'SS_RoadNode.shp',\n",
       "  'SS_RoadNode.shx',\n",
       "  'ST_MotorwayJunction.dbf',\n",
       "  'ST_MotorwayJunction.prj',\n",
       "  'ST_MotorwayJunction.shp',\n",
       "  'ST_MotorwayJunction.shx',\n",
       "  'ST_RoadLink.dbf',\n",
       "  'ST_RoadLink.prj',\n",
       "  'ST_RoadLink.shp',\n",
       "  'ST_RoadLink.shx',\n",
       "  'ST_RoadNode.dbf',\n",
       "  'ST_RoadNode.prj',\n",
       "  'ST_RoadNode.shp',\n",
       "  'ST_RoadNode.shx',\n",
       "  'SU_MotorwayJunction.dbf',\n",
       "  'SU_MotorwayJunction.prj',\n",
       "  'SU_MotorwayJunction.shp',\n",
       "  'SU_MotorwayJunction.shx',\n",
       "  'SU_RoadLink.dbf',\n",
       "  'SU_RoadLink.prj',\n",
       "  'SU_RoadLink.shp',\n",
       "  'SU_RoadLink.shx',\n",
       "  'SU_RoadNode.dbf',\n",
       "  'SU_RoadNode.prj',\n",
       "  'SU_RoadNode.shp',\n",
       "  'SU_RoadNode.shx',\n",
       "  'SV_RoadLink.dbf',\n",
       "  'SV_RoadLink.prj',\n",
       "  'SV_RoadLink.shp',\n",
       "  'SV_RoadLink.shx',\n",
       "  'SV_RoadNode.dbf',\n",
       "  'SV_RoadNode.prj',\n",
       "  'SV_RoadNode.shp',\n",
       "  'SV_RoadNode.shx',\n",
       "  'SW_RoadLink.dbf',\n",
       "  'SW_RoadLink.prj',\n",
       "  'SW_RoadLink.shp',\n",
       "  'SW_RoadLink.shx',\n",
       "  'SW_RoadNode.dbf',\n",
       "  'SW_RoadNode.prj',\n",
       "  'SW_RoadNode.shp',\n",
       "  'SW_RoadNode.shx',\n",
       "  'SX_MotorwayJunction.dbf',\n",
       "  'SX_MotorwayJunction.prj',\n",
       "  'SX_MotorwayJunction.shp',\n",
       "  'SX_MotorwayJunction.shx',\n",
       "  'SX_RoadLink.dbf',\n",
       "  'SX_RoadLink.prj',\n",
       "  'SX_RoadLink.shp',\n",
       "  'SX_RoadLink.shx',\n",
       "  'SX_RoadNode.dbf',\n",
       "  'SX_RoadNode.prj',\n",
       "  'SX_RoadNode.shp',\n",
       "  'SX_RoadNode.shx',\n",
       "  'SY_RoadLink.dbf',\n",
       "  'SY_RoadLink.prj',\n",
       "  'SY_RoadLink.shp',\n",
       "  'SY_RoadLink.shx',\n",
       "  'SY_RoadNode.dbf',\n",
       "  'SY_RoadNode.prj',\n",
       "  'SY_RoadNode.shp',\n",
       "  'SY_RoadNode.shx',\n",
       "  'SZ_RoadLink.dbf',\n",
       "  'SZ_RoadLink.prj',\n",
       "  'SZ_RoadLink.shp',\n",
       "  'SZ_RoadLink.shx',\n",
       "  'SZ_RoadNode.dbf',\n",
       "  'SZ_RoadNode.prj',\n",
       "  'SZ_RoadNode.shp',\n",
       "  'SZ_RoadNode.shx',\n",
       "  'TA_MotorwayJunction.dbf',\n",
       "  'TA_MotorwayJunction.prj',\n",
       "  'TA_MotorwayJunction.shp',\n",
       "  'TA_MotorwayJunction.shx',\n",
       "  'TA_RoadLink.dbf',\n",
       "  'TA_RoadLink.prj',\n",
       "  'TA_RoadLink.shp',\n",
       "  'TA_RoadLink.shx',\n",
       "  'TA_RoadNode.dbf',\n",
       "  'TA_RoadNode.prj',\n",
       "  'TA_RoadNode.shp',\n",
       "  'TA_RoadNode.shx',\n",
       "  'TF_RoadLink.dbf',\n",
       "  'TF_RoadLink.prj',\n",
       "  'TF_RoadLink.shp',\n",
       "  'TF_RoadLink.shx',\n",
       "  'TF_RoadNode.dbf',\n",
       "  'TF_RoadNode.prj',\n",
       "  'TF_RoadNode.shp',\n",
       "  'TF_RoadNode.shx',\n",
       "  'TG_RoadLink.dbf',\n",
       "  'TG_RoadLink.prj',\n",
       "  'TG_RoadLink.shp',\n",
       "  'TG_RoadLink.shx',\n",
       "  'TG_RoadNode.dbf',\n",
       "  'TG_RoadNode.prj',\n",
       "  'TG_RoadNode.shp',\n",
       "  'TG_RoadNode.shx',\n",
       "  'TL_MotorwayJunction.dbf',\n",
       "  'TL_MotorwayJunction.prj',\n",
       "  'TL_MotorwayJunction.shp',\n",
       "  'TL_MotorwayJunction.shx',\n",
       "  'TL_RoadLink.dbf',\n",
       "  'TL_RoadLink.prj',\n",
       "  'TL_RoadLink.shp',\n",
       "  'TL_RoadLink.shx',\n",
       "  'TL_RoadNode.dbf',\n",
       "  'TL_RoadNode.prj',\n",
       "  'TL_RoadNode.shp',\n",
       "  'TL_RoadNode.shx',\n",
       "  'TM_RoadLink.dbf',\n",
       "  'TM_RoadLink.prj',\n",
       "  'TM_RoadLink.shp',\n",
       "  'TM_RoadLink.shx',\n",
       "  'TM_RoadNode.dbf',\n",
       "  'TM_RoadNode.prj',\n",
       "  'TM_RoadNode.shp',\n",
       "  'TM_RoadNode.shx',\n",
       "  'TQ_MotorwayJunction.dbf',\n",
       "  'TQ_MotorwayJunction.prj',\n",
       "  'TQ_MotorwayJunction.shp',\n",
       "  'TQ_MotorwayJunction.shx',\n",
       "  'TQ_RoadLink.dbf',\n",
       "  'TQ_RoadLink.prj',\n",
       "  'TQ_RoadLink.shp',\n",
       "  'TQ_RoadLink.shx',\n",
       "  'TQ_RoadNode.dbf',\n",
       "  'TQ_RoadNode.prj',\n",
       "  'TQ_RoadNode.shp',\n",
       "  'TQ_RoadNode.shx',\n",
       "  'TR_MotorwayJunction.dbf',\n",
       "  'TR_MotorwayJunction.prj',\n",
       "  'TR_MotorwayJunction.shp',\n",
       "  'TR_MotorwayJunction.shx',\n",
       "  'TR_RoadLink.dbf',\n",
       "  'TR_RoadLink.prj',\n",
       "  'TR_RoadLink.shp',\n",
       "  'TR_RoadLink.shx',\n",
       "  'TR_RoadNode.dbf',\n",
       "  'TR_RoadNode.prj',\n",
       "  'TR_RoadNode.shp',\n",
       "  'TR_RoadNode.shx',\n",
       "  'TV_RoadLink.dbf',\n",
       "  'TV_RoadLink.prj',\n",
       "  'TV_RoadLink.shp',\n",
       "  'TV_RoadLink.shx',\n",
       "  'TV_RoadNode.dbf',\n",
       "  'TV_RoadNode.prj',\n",
       "  'TV_RoadNode.shp',\n",
       "  'TV_RoadNode.shx'],\n",
       " '_i22': '#Natural England data\\nNE_Data = {\\n    \"NE_Peat\" : {\"url\":\"https://opendata.arcgis.com/datasets/55b21c31a61c4292a465d7618d831eb8_0.geojson\", \"Type\":\"Peat\", \"Name\":\"PCLASSDESC\", \"Details_Str\":\"Type\", \"geometry\":\"geometry\"},\\n    \"NE_SSI\" : {\"url\":\"https://opendata.arcgis.com/datasets/03fd7a2f8e4e4346bf41ce7879153949_0.geojson\", \"Type\":\"SiteOfScientificInterest\", \"Name\":\"SSSI_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_LocalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/b1d690ac6dd54c15bdd2d341b686ecd7_0.geojson\", \"Type\":\"LocalNatureReserve\", \"Name\":\"LNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_NationalNatureReserve\" : {\"url\":\"https://opendata.arcgis.com/datasets/ab7bfd86f5b347df8d47fc9bfab80caf_0.geojson\", \"Type\":\"NationalNatureReserve\", \"Name\":\"NNR_NAME\", \"Details_Str\":\"OBJECTID\", \"geometry\":\"geometry\"},\\n    \"NE_SpecialAreaOfConvervation\" : {\"url\":\"https://opendata.arcgis.com/datasets/e4142658906c498fa37f0a20d3fdfcff_0.geojson\", \"Type\":\"SpecialAreaOfConservation\", \"Name\":\"SAC_NAME\", \"Details_Str\":\"SAC_CODE\", \"geometry\":\"geometry\"},\\n    \"NE_AncientWoodland\" : {\"url\":\"https://opendata.arcgis.com/datasets/a14064ca50e242c4a92d020764a6d9df_0.geojson\", \"Type\":\"AncientWoodland\", \"Name\":\"NAME\", \"Details_Str\":\"THEMNAME\", \"geometry\":\"geometry\"}\\n}\\n\\ndef get_NE_Data(details):\\n    url = details[\"url\"]\\n\\n    usecols = [details[\"Name\"], details[\"Details_Str\"], details[\"geometry\"]]\\n\\n    gdf = gpd.read_file(url).to_crs(Main_CRS).loc[:, usecols]\\n\\n\\n    gdf = gdf.rename(columns={details[\"Name\"]: \"Name\"})\\n    gdf = gdf.rename(columns={details[\"Details_Str\"]: \"Details_Str\"})\\n    gdf = gdf.rename(columns={details[\"geometry\"]: \"geometry\"})\\n    gdf[\"Type\"] = details[\"Type\"]\\n\\n    return gdf\\n\\nfor k in NE_Data.keys():\\n    v = NE_Data[k]\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, get_NE_Data(v)], ignore_index=True) )\\n\\ndel NE_Data',\n",
       " 'get_NE_Data': <function __main__.get_NE_Data(details)>,\n",
       " '_i23': '#Historic England Data\\n \\npath = root_path + \"HistoricEngland/Conservation Areas/20210922_Conservation_Areas_INSPIRE_dataset.shp\"\\nHE_gdf = gpd.read_file(path).loc[:,[\"NAME\", \"geometry\"]].to_crs(Main_CRS)\\nHE_gdf = HE_gdf.rename(columns={\"NAME\": \"Name\"})\\nHE_gdf[\"Type\"] = \"ConservationArea\"\\n\\npath = root_path + \"HistoricEngland/Listed Buildings/ListedBuildings_12Jan2022.shp\"\\nLB_gdf = gpd.read_file(path).loc[:,[\"Name\", \"Grade\", \"geometry\"]].to_crs(Main_CRS)\\nLB_gdf = LB_gdf.rename(columns={\"Grade\": \"Details_Str\"})\\nLB_gdf[\"Type\"] = \"Listed Buildings\"\\n\\nHE_gdf = gpd.GeoDataFrame(pd.concat( [HE_gdf, LB_gdf], ignore_index=True) )\\ndel LB_gdf\\n\\nraw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, HE_gdf], ignore_index=True) )\\ndel HE_gdf, path',\n",
       " '_i24': '#Met office data for 2020\\nmet_data = {\\n    \"rainfall\":[\"MetOffice/rainfall_hadukgrid_uk_1km_ann_202001-202012.nc\", \"TotalRainfall_mm_2020\"],\\n    \"snowLying\":[\"MetOffice/snowLying_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Snow_Days_2020\"],\\n    \"sun\":[\"MetOffice/sun_hadukgrid_uk_1km_ann_202001-202012.nc\", \"Sunlight_h_2020\"],\\n    \"tas\":[\"MetOffice/tas_hadukgrid_uk_1km_ann_202001-202012.nc\", \"AverageTemperature_C_2020\"],\\n    \"groundfrost\":[\"MetOffice/groundfrost_hadukgrid_uk_1km_ann_202001-202012.nc\", \"GroundFrost_Days_2020\"]\\n    }\\n\\ndef import_met(path, col, name):\\n    pth = root_path +path\\n    dnc = xr.open_dataset(pth)  \\n    df = dnc.to_dataframe()\\n\\n    df = df.dropna().reset_index().loc[:, [\"projection_y_coordinate\", \"projection_x_coordinate\", col, \"bnds\"]]\\n    df = df.loc[df[\"bnds\"]==0,:].drop(columns=\"bnds\").reset_index(drop=True)\\n    df = df.rename(columns={col: \"Details_Float\"})\\n    df[\"Name\"] = name\\n    df[\"Type\"] = \"MetOffice\"\\n\\n    #Convert to Geopandas dataframe\\n    x_points = df[\"projection_x_coordinate\"].to_numpy()\\n    y_points = df[\"projection_y_coordinate\"].to_numpy()\\n\\n    gdf = gpd.GeoDataFrame(df.loc[:,[\"Details_Float\", \"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\\n\\n    return gdf\\n\\nfor k in met_data.keys():\\n    raw_gdf = gpd.GeoDataFrame(pd.concat( [raw_gdf, import_met(met_data[k][0], k, met_data[k][1])], ignore_index=True) )\\n\\ndel met_data',\n",
       " 'import_met': <function __main__.import_met(path, col, name)>,\n",
       " '_i25': \"raw_gdf.to_pickle(root_path + 'WorkingData/' + 'raw_gdf.pickle')\",\n",
       " '_i26': 'locals()'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78a45328dcddc684fcc7e97834e2f9ed1c731cb5e93878b3fdd5359bd832b4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('GeoData': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
