{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/GeoData/\"\n",
    "Main_CRS = \"EPSG:27700\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into a standard structrue for the geopandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Import_Points(infile, inputs):\n",
    "    print(inputs[\"Type\"])\n",
    "    #Get the before and after columns names\n",
    "    usecols = list(filter(None,list(inputs.values())[1:8]))\n",
    "    col_names = list({key: value for key, value in inputs.items() if value in usecols}.keys())\n",
    "\n",
    "    #Get the data types\n",
    "    t_dtypes = {key: value for key, value in dtypes.items() if key in col_names}\n",
    "    dtypes_in = {inputs[k]:v for k, v in t_dtypes.items()}\n",
    "    dtype_out = {key: value for key, value in dtypes.items() if key in col_names}\n",
    "\n",
    "    #Import the data\n",
    "    df = pd.read_csv(infile, usecols=usecols, encoding = \"ISO-8859-1\", dtype=dtypes_in) \n",
    "\n",
    "    #Reformat ready for next step\n",
    "    df.columns = col_names\n",
    "    df = df.astype(dtype_out)\n",
    "\n",
    "    df[\"Type\"] = inputs[\"Type\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data\n",
    "rail_infile = root_path + \"NaPTANcsv/RailReferences.csv\"\n",
    "rail_inputs = {\"Type\":\"RailwayStations\", \"Name\":\"StationName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "ferry_infile = root_path + \"NaPTANcsv/FerryReferences.csv\"\n",
    "ferry_inputs = {\"Type\":\"FerryTerminals\", \"Name\":\"Name\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "bus_infile = root_path + \"NaPTANcsv/Stops.csv\"\n",
    "bus_inputs = {\"Type\":\"BusStops\", \"Name\":\"CommonName\", \"Easting\":\"Easting\", \"Northing\":\"Northing\"}\n",
    "\n",
    "acc2020_infile = root_path + \"RoadSafety/dft-road-casualty-statistics-accident-2020.csv\"\n",
    "acc2020_inputs = {\"Type\":\"RoadAccidents\", \"Name\":\"accident_reference\", \"Easting\":\"location_easting_osgr\", \"Northing\":\"location_northing_osgr\"}\n",
    "\n",
    "NSPL_infile = root_path + \"NSPL_NOV_2020_UK/Data/NSPL_NOV_2020_UK.csv\"\n",
    "NSPL_inputs = {\"Type\":\"NSPL\", \"Name\":\"pcd\", \"Easting\":\"oseast1m\", \"Northing\":\"osnrth1m\"}\n",
    "\n",
    "#NSUL_Infile defined below\n",
    "NSUL_inputs = {\"Type\":\"NSUL\", \"Name\":\"uprn\", \"Easting\":\"gridgb1e\", \"Northing\":\"gridgb1n\", \"Details_Str\":\"pcds\"}\n",
    "\n",
    "point_pairs = [[rail_infile, rail_inputs],\n",
    "[ferry_infile, ferry_inputs],\n",
    "[bus_infile, bus_inputs],\n",
    "[acc2020_infile, acc2020_inputs],\n",
    "[NSPL_infile,NSPL_inputs]]\n",
    "\n",
    "NSUL_Pairs = [[root_path + \"NSUL_OCT_2020/Data/\" + str(i), NSUL_inputs] for i in os.listdir(\"D:/GeoData/NSUL_OCT_2020/Data\") if \".csv\" in i]\n",
    "\n",
    "point_pairs = point_pairs\n",
    "# + NSUL_Pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate the standard structure\n",
    "Points_of_Interest = pd.DataFrame({\"Type\":\"\", \"Name\":\"\", \"Details_Str\":\"\", \"Details_Float\":0, \"Easting\":0, \"Northing\":0}, index=[0])\n",
    "dtypes = {\"Type\":\"str\", \"Name\":\"str\", \"Details_Str\":\"str\", \"Details_Float\":np.float64, \"Easting\":np.float64, \"Northing\":np.float64}\n",
    "Points_of_Interest = Points_of_Interest.astype(dtypes)\n",
    "Points_of_Interest = Points_of_Interest.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RailwayStations\n",
      "FerryTerminals\n",
      "BusStops\n",
      "RoadAccidents\n",
      "NSPL\n"
     ]
    }
   ],
   "source": [
    "for l in point_pairs:\n",
    "    Points_of_Interest = Points_of_Interest.append(Import_Points(l[0], l[1]), ignore_index=True)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"NSPL\", \"Name\"] = Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"NSPL\", \"Name\"].astype(str).str.replace(\" \",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the land registry data, merge on the postcodes from NSPL data then add to the main file\n",
    "LReg_Names = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', \n",
    "              'Old_New', 'Duration', 'PAON', 'SAON', 'Street', 'Locality', 'Town_City', 'District', 'County', \n",
    "              'PPDCategory_Type', 'Record_Status_monthly_file_only']\n",
    "\n",
    "usecols = ['Transaction_unique_identifier', 'Price', 'Date_of_Transfer', 'Postcode', 'Property_Type', 'Old_New', 'Duration']\n",
    "\n",
    "LReg_Data2020 = pd.read_csv(root_path + \"LandReg/pp-2020.csv\", names=LReg_Names, usecols=usecols) \n",
    "LReg_Data2021 = pd.read_csv(root_path + \"LandReg/pp-2021.csv\", names=LReg_Names, usecols=usecols)\n",
    "\n",
    "LReg_Data = pd.concat([LReg_Data2020, LReg_Data2021])\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "LReg_Data[\"Postcode\"] = LReg_Data[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "LReg_Data = LReg_Data.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"NSPL\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "LReg_Data[\"Details_Str\"] = np.column_stack((LReg_Data[\"Date_of_Transfer\"].to_numpy(),\n",
    "    LReg_Data[\"Property_Type\"].to_numpy(), \n",
    "    LReg_Data[\"Old_New\"].to_numpy(), \n",
    "    LReg_Data[\"Duration\"].to_numpy())\n",
    "    ).tolist()\n",
    "\n",
    "LReg_Data[\"Details_Float\"] = LReg_Data[\"Price\"]\n",
    "LReg_Data[\"Name\"] = LReg_Data[\"Transaction_unique_identifier\"]\n",
    "LReg_Data[\"Type\"] = \"LReg\"\n",
    "LReg_Data = LReg_Data.loc[:, [\"Type\", \"Name\", \"Details_Str\", \"Details_Float\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(LReg_Data, ignore_index=True)\n",
    "del LReg_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firestation Data\n",
    "FireStations = pd.read_excel(root_path + \"Fire_data/\" + \"fire-stations-dataset-121120.ods\", \n",
    "                         engine=\"odf\",\n",
    "                        sheet_name = \"STATIONS\")\n",
    "\n",
    "FireStations[\"Type\"] = \"FireStations\"\n",
    "FireStations[\"Name\"] = FireStations[\"STATION_NAME\"]\n",
    "FireStations[\"Easting\"] = FireStations[\"STATION_EASTING\"]\n",
    "FireStations[\"Northing\"] = FireStations[\"STATION_NORTHING\"]\n",
    "FireStations = FireStations.loc[:, [\"Type\", \"Name\", \"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(FireStations, ignore_index=True)\n",
    "del FireStations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schools Data\n",
    "usecols=[\"Postcode\", \"EstablishmentName\", \"EstablishmentTypeGroup (name)\"]\n",
    "Schools =pd.read_csv(root_path+\"Schools/\"+\"results.csv\", encoding = \"ISO-8859-1\", low_memory=False, usecols=usecols)\n",
    "\n",
    "#remove the spaces from the postcode\n",
    "Schools[\"Postcode\"] = Schools[\"Postcode\"].astype(str).str.replace(\" \",\"\")\n",
    "\n",
    "Schools = Schools.merge(Points_of_Interest.loc[Points_of_Interest[\"Type\"]==\"NSPL\", [\"Name\", \"Easting\", \"Northing\"]], left_on=\"Postcode\", right_on=\"Name\")\n",
    "\n",
    "Schools[\"Type\"] = \"Schools\"\n",
    "Schools[\"Details_Str\"] = Schools[\"EstablishmentTypeGroup (name)\"]\n",
    "Schools[\"Name\"] = Schools[\"EstablishmentName\"]\n",
    "Schools = Schools.loc[:, [\"Type\", \"Name\", \"Details_Str\",\"Easting\", \"Northing\"]]\n",
    "\n",
    "Points_of_Interest = Points_of_Interest.append(Schools, ignore_index=True)\n",
    "del Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We know that some of the easings and northings are on the Irish grid, split these out\n",
    "NI_Mask_1 = ((Points_of_Interest[\"Type\"]==\"NSPL\") & (Points_of_Interest[\"Name\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask_2 =  ((Points_of_Interest[\"Type\"]==\"NSUL\") & (Points_of_Interest[\"Details_Str\"].str.slice(stop=2)==\"BT\")).to_numpy()\n",
    "NI_Mask = np.logical_or(NI_Mask_1, NI_Mask_2)\n",
    "\n",
    "NI = Points_of_Interest.loc[NI_Mask,:]\n",
    "Points_of_Interest = Points_of_Interest.loc[~NI_Mask,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Geopandas dataframe\n",
    "x_points = Points_of_Interest[\"Easting\"].to_numpy()\n",
    "y_points = Points_of_Interest[\"Northing\"].to_numpy()\n",
    "\n",
    "points_gdf = gpd.GeoDataFrame(Points_of_Interest.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=Main_CRS)\n",
    "del Points_of_Interest\n",
    "\n",
    "#Do the same for NI, and convert to the appropriate crs\n",
    "x_points = NI[\"Easting\"].to_numpy()\n",
    "y_points = NI[\"Northing\"].to_numpy()\n",
    "\n",
    "NI_gdf = gpd.GeoDataFrame(NI.loc[:,[\"Type\", \"Name\", \"Details_Str\", \"Details_Float\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:29902\")\n",
    "NI_gdf = NI_gdf.to_crs(Main_CRS)\n",
    "del NI\n",
    "\n",
    "#Re-combine now they are on the same coordinates\n",
    "points_gdf = gpd.GeoDataFrame( pd.concat( [points_gdf, NI_gdf], ignore_index=True) )\n",
    "del NI_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Police Crime data\n",
    "usecols = [\"Crime type\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Crime type\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"street.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "Crimes = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\n",
    "\n",
    "for file in infile[1:]:\n",
    "    Crimes=Crimes.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\n",
    "\n",
    "Crimes = Crimes.dropna()\n",
    "Crimes = Crimes.rename(columns={\"Crime type\": \"Name\"})\n",
    "Crimes[\"Type\"] = \"Crimes\"\n",
    "\n",
    "#Police Stop and Search Data\n",
    "usecols = [\"Object of search\", \"Longitude\", \"Latitude\"]\n",
    "in_dtypes = {\"Object of search\":\"str\", \"Longitude\":np.float64, \"Latitude\":np.float64}\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk(\"D:/GeoData/PoliceData/\"):\n",
    "    for name in files:\n",
    "        if name.endswith((\"search.csv\")):\n",
    "            file_list = file_list + [str(root) + \"/\" + str(name)]\n",
    "\n",
    "infile = file_list[0]\n",
    "\n",
    "StopAndSearch = pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols)\n",
    "\n",
    "for file in infile[1:]:\n",
    "    StopAndSearch=StopAndSearch.append(pd.read_csv(infile, encoding = \"ISO-8859-1\", dtype=in_dtypes, usecols=usecols), ignore_index=True)\n",
    "\n",
    "StopAndSearch = StopAndSearch.dropna()\n",
    "StopAndSearch = StopAndSearch.rename(columns={\"Object of search\": \"Name\"})\n",
    "StopAndSearch[\"Type\"] = \"StopAndSearch\"\n",
    "\n",
    "Crimes = Crimes.append(StopAndSearch,ignore_index=True)\n",
    "del StopAndSearch\n",
    "\n",
    "#Convert to Geopandas dataframe\n",
    "x_points = Crimes[\"Longitude\"].to_numpy()\n",
    "y_points = Crimes[\"Latitude\"].to_numpy()\n",
    "\n",
    "crimes_gdf = gpd.GeoDataFrame(Crimes.loc[:,[\"Type\", \"Name\"]], geometry=gpd.points_from_xy(x_points, y_points), crs=\"EPSG:4326\")\n",
    "crimes_gdf = crimes_gdf.to_crs(Main_CRS)\n",
    "crimes_gdf\n",
    "\n",
    "del Crimes\n",
    "\n",
    "#combine with the main points gdf\n",
    "points_gdf = gpd.GeoDataFrame( pd.concat( [points_gdf, crimes_gdf], ignore_index=True) )\n",
    "del crimes_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf.to_pickle(root_path + 'WorkingData/' + 'points_gdf.pickle')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78a45328dcddc684fcc7e97834e2f9ed1c731cb5e93878b3fdd5359bd832b4d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('GeoData': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
